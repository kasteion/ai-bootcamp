{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb6d23cb",
   "metadata": {},
   "source": [
    "# LLM Judges for Agent Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab7264",
   "metadata": {},
   "source": [
    "We'll implement an LLM-based evaluation system to systematically assess our agent's performance using the results we collected in the previous lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a825c83",
   "metadata": {},
   "source": [
    "## Setting Up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9698843a",
   "metadata": {},
   "source": [
    "First, let's load the data we have already prepared for evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3746b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('sample_eval_rows.bin', 'rb') as f_in:\n",
    "    rows = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34527c2c",
   "metadata": {},
   "source": [
    "## Initial Evaluation Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4bf91b",
   "metadata": {},
   "source": [
    "Let's start with a simple evaluation prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64419a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent’s answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user’s instructions (in <INSTRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do  \n",
    "- answer_relevant: The response directly addresses the user’s question  \n",
    "- answer_clear: The answer is clear and correct  \n",
    "- answer_citations: The response includes proper citations or sources when required  \n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: Is the search tool invoked? \n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104464a8",
   "metadata": {},
   "source": [
    "And structured output classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804330c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    reasoning: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897549f1",
   "metadata": {},
   "source": [
    "We run it like any other Pydantic AI agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f8414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "eval_agent = Agent(\n",
    "    name='eval_agent',\n",
    "    model='gpt-5-mini',\n",
    "    instructions=evaluation_prompt,\n",
    "    output_type=EvaluationChecklist\n",
    ")\n",
    "\n",
    "eval_agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2161911a",
   "metadata": {},
   "source": [
    "This is a good start but has a potential problem: the model can hallucinate check names and create inconsistent evaluations. We need a more structured approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958dc63e",
   "metadata": {},
   "source": [
    "## Structured Evaluation with Pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3526443",
   "metadata": {},
   "source": [
    "To ensure consistency and prevent hallucination, we'll enforce the evaluation structure with Enums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f4197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class CheckName(str, Enum):\n",
    "    instructions_follow = \"instructions_follow\"\n",
    "    instructions_avoid = \"instructions_avoid\" \n",
    "    answer_relevant = \"answer_relevant\"\n",
    "    answer_clear = \"answer_clear\"\n",
    "    answer_citations = \"answer_citations\"\n",
    "    completeness = \"completeness\"\n",
    "    tool_call_search = \"tool_call_search\"\n",
    "\n",
    "CHECK_DESCRIPTIONS = {\n",
    "    CheckName.instructions_follow: \"The agent followed the user's instructions (in <INSTRUCTIONS>)\",\n",
    "    CheckName.instructions_avoid: \"The agent avoided doing things it was told not to do\",\n",
    "    CheckName.answer_relevant: \"The response directly addresses the user's question\",\n",
    "    CheckName.answer_clear: \"The answer is clear and correct\",\n",
    "    CheckName.answer_citations: \"The response includes proper citations or sources when required\",\n",
    "    CheckName.completeness: \"The response is complete and covers all key aspects of the request\",\n",
    "    CheckName.tool_call_search: \"Is the search tool invoked?\"\n",
    "}\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: CheckName = Field(description=\"The type of evaluation check\")\n",
    "    reasoning: str = Field(description=\"The reasoning behind the check result\")\n",
    "    check_pass: bool = Field(description=\"Whether the check passed (True) or failed (False)\")\n",
    "    \n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck] = Field(description=\"List of all evaluation checks\")\n",
    "    summary: str = Field(description=\"Evaluation summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f2440f",
   "metadata": {},
   "source": [
    "Now we can generate the checklist instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b67da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_checklist_text():\n",
    "    checklist_items = []\n",
    "    for check_name in CheckName:\n",
    "        description = CHECK_DESCRIPTIONS[check_name]\n",
    "        checklist_items.append(f\"- {check_name.value}: {description}\")\n",
    "    return \"\\n\".join(checklist_items)\n",
    "\n",
    "\n",
    "eval_instructions = f\"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent’s answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "{generate_checklist_text()}\n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4512569",
   "metadata": {},
   "source": [
    "And use it for running the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c9b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "eval_agent = Agent(\n",
    "    name='eval_agent',\n",
    "    model='gpt-5-mini', # it's recommended to use a different model\n",
    "    instructions=evaluation_prompt,\n",
    "    output_type=EvaluationChecklist\n",
    ")\n",
    "\n",
    "eval_agent.run(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6614e19",
   "metadata": {},
   "source": [
    "## Running the Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ab9e3b",
   "metadata": {},
   "source": [
    "Now let's prepare the user prompt for each of the records we will evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6366e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<LOG>{log}</LOG>\n",
    "\"\"\".strip()\n",
    "\n",
    "def format_prompt(rec):\n",
    "    answer = rec['original_result'].output.format_article()\n",
    "    \n",
    "    logs = '\\n'.join(json.dumps(l) for l in rec['messages'])\n",
    "                     \n",
    "    return user_prompt_format.format(\n",
    "        instructions=agent._instructions,\n",
    "        question=rec['question'],\n",
    "        answer=answer,\n",
    "        log=logs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9112f",
   "metadata": {},
   "source": [
    "Let's run it! We'll use the same code for parallel execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "async def map_progress(seq, f, max_concurrency=6):\n",
    "    \"\"\"Asynchronously map async function f over seq with progress bar.\"\"\"\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "    async def run(el):\n",
    "        async with semaphore:\n",
    "            return await f(el)\n",
    "\n",
    "    # create one coroutine per element\n",
    "    coros = [run(el) for el in seq]\n",
    "\n",
    "    # turn them into tasks that complete as they finish\n",
    "    completed = asyncio.as_completed(coros)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for coro in tqdm(completed, total=len(seq)):\n",
    "        result = await coro\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f869559",
   "metadata": {},
   "source": [
    "Create a function we'll apply to each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_record(rec):\n",
    "    user_prompt = format_prompt(rec)\n",
    "    result = await eval_agent.run(user_prompt)\n",
    "    return rec, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb320cd3",
   "metadata": {},
   "source": [
    "And run the evals!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99505cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = await map_progress(rows, evaluate_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ecdcd",
   "metadata": {},
   "source": [
    "Let's see how much it cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6547050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.pricing import PricingConfig\n",
    "pricing = PricingConfig()\n",
    "\n",
    "def calculate_cost(model, all_results):\n",
    "    total_input = 0\n",
    "    total_output = 0\n",
    "    \n",
    "    for q, result in all_results:\n",
    "        if result is None:\n",
    "            continue\n",
    "\n",
    "        usage = result.usage()\n",
    "        total_input = total_input + usage.input_tokens\n",
    "        total_output = total_output + usage.output_tokens\n",
    "    \n",
    "    return pricing.calculate_cost(model, total_input, total_output)\n",
    "\n",
    "calculate_cost('gpt-5-mini', all_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f05da83",
   "metadata": {},
   "source": [
    "So, the total cost of one eval:\n",
    "\n",
    "- 0.12 for running the agent\n",
    "- 0.22 for evaluating\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a497fe56",
   "metadata": {},
   "source": [
    "## Displaying the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e57e37",
   "metadata": {},
   "source": [
    "Now let's display the results (and save them too):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471dfa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_results = []\n",
    "\n",
    "for rec, result in all_results:\n",
    "    eval_row = rec.copy()\n",
    "    eval_result = result.output\n",
    "    checks = {c.check_name.value: c.check_pass for c in eval_result.checklist}\n",
    "    eval_row.update(checks)\n",
    "    eval_row['summary'] = eval_result.summary\n",
    "    eval_results.append(eval_row)\n",
    "\n",
    "\n",
    "df_eval = pd.DataFrame(eval_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c611c31",
   "metadata": {},
   "source": [
    "Now we show only the columns with evals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b5a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_columns = [check_name.value for check_name in CheckName]\n",
    "df_eval[eval_columns].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b11d0",
   "metadata": {},
   "source": [
    "We get something like that:\n",
    "\n",
    "```bash\n",
    "instructions_follow    0.36\n",
    "instructions_avoid     0.60\n",
    "answer_relevant        0.98\n",
    "answer_clear           0.50\n",
    "answer_citations       0.58\n",
    "completeness           0.10\n",
    "tool_call_search       0.96\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac64e9e3",
   "metadata": {},
   "source": [
    "It's not perfect but at least we know where we stand now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25381a35",
   "metadata": {},
   "source": [
    "## New Check: answer_match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12acc069",
   "metadata": {},
   "source": [
    "In addition to the checks we did before, we can add another one: `answer_match`. We will ask the judge to compare the answer generated by the agent with the original article. If the answers match, it should be evaluated to true.\n",
    "\n",
    "For that, we will need to add a way to fetch the original file. Let's create an index for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b1bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs\n",
    "\n",
    "github_data = docs.read_github_data()\n",
    "parsed_data = docs.parse_data(github_data)\n",
    "\n",
    "file_index = {d['filename']: d['content'] for d in parsed_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1381c06",
   "metadata": {},
   "source": [
    "Now we can use it to get the content of any file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a31a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_index['metrics/all_metrics.mdx']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa325b",
   "metadata": {},
   "source": [
    "Let's update the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06735d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<REFERENCE>{referece}</REFERENCE>\n",
    "<LOG>{log}</LOG>\n",
    "\"\"\".strip()\n",
    "\n",
    "def format_prompt(rec):\n",
    "    original_filename = rec['original_question']['filename']\n",
    "    reference = file_index[original_filename]\n",
    "\n",
    "    answer = rec['original_result'].output.format_article()\n",
    "\n",
    "    logs = '\\n'.join(json.dumps(l) for l in rec['messages'])\n",
    "\n",
    "    return user_prompt_format.format(\n",
    "        instructions=agent._instructions,\n",
    "        question=rec['question'],\n",
    "        answer=answer,\n",
    "        reference=reference,\n",
    "        log=logs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e4978f",
   "metadata": {},
   "source": [
    "Our prompt:\n",
    "\n",
    "```text\n",
    "Use this checklist to evaluate the quality of an AI agent’s answer\n",
    "(<ANSWER>) to a user question (<QUESTION>). We also include the\n",
    "entire log (<LOG>) for analysis. In <REFERENCE> you will see\n",
    "the file, from which the user question was generated. \n",
    "```\n",
    "\n",
    "And the new checklist item:\n",
    "\n",
    "```text\n",
    "- answer_match: The ANSWER is similar to the REFERENCE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb72a15",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37fc1c6",
   "metadata": {},
   "source": [
    "This systematic evaluation approach provides objective metrics to guide agent improvements and track progress over time.\n",
    "\n",
    "The cost of $0.34 per evaluation cycle is also reasonable, so we can use this approach to tune our agent. For example, to select the best chunking approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
