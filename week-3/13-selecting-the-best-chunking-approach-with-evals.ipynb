{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07eee7c8",
   "metadata": {},
   "source": [
    "# Selecting the Best Chunking Approach with Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fa308",
   "metadata": {},
   "source": [
    "So far we've selected chunking parameters pretty arbitrarily. While the relevance was okay, we can be much smarter about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d177913",
   "metadata": {},
   "source": [
    "## Key Factors in Chunking Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da1f073",
   "metadata": {},
   "source": [
    "When selecting chunking parameters, we need to consider three critical factors:\n",
    "\n",
    "- Hit rate: How often we find relevant documents\n",
    "\n",
    "- Time and cost: Processing efficiency and token usage\n",
    "\n",
    "- Relevance: Quality of retrieved information\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cbc2b4",
   "metadata": {},
   "source": [
    "# The Trade-off Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00269c47",
   "metadata": {},
   "source": [
    "We can easily increase hit rate by retrieving more chunks. But this results in higher time and cost.\n",
    "\n",
    "We can decrease cost by making chunks smaller. But then we won't retrieve complete information, and our code examples will be incomplete.\n",
    "\n",
    "We need to balance all these aspects. When we increase the size of chunks, we increase both time and cost. That's why we can use cost as a proxy for efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c9a4a2",
   "metadata": {},
   "source": [
    "## Defining Our Optimization Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b918e",
   "metadata": {},
   "source": [
    "At the end, we want the method that:\n",
    "\n",
    "- Has the highest hit rate (H)\n",
    "- Has the highest relevance (R)\n",
    "- Has the smallest cost (C)\n",
    "\n",
    "Let's define our metrics:\n",
    "\n",
    "- H = average hit rate (0-1)\n",
    "- R = average relevance (0-1)\n",
    "- C = average cost in cents\n",
    "\n",
    "We can combine these into one objective function:\n",
    "\n",
    "O = f(H, R, C)\n",
    "\n",
    "We want to maximize H and R while minimizing C.\n",
    "\n",
    "One option for the function f:\n",
    "\n",
    "f = H^α × R^β / C^γ\n",
    "\n",
    "This gives us the best hit rate and relevance we can get per unit of cost.\n",
    "\n",
    "Since H and R are already normalized between 0 and 1, we don't need extra scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069eabba",
   "metadata": {},
   "source": [
    "## Systematic Testing Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9389a616",
   "metadata": {},
   "source": [
    "Our testing strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c6732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [1000, 2000, 3000, 5000]\n",
    "steps = [1000, 2000, 3000]\n",
    "top_ks = [5, 10, 15]\n",
    "\n",
    "results = []\n",
    "\n",
    "for size in sizes:\n",
    "    for step in steps:\n",
    "        for top_k in top_ks:\n",
    "            evaluate_agent(size, step, top_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f772c4c",
   "metadata": {},
   "source": [
    "Problem: Too many options would take excessive time to test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892c4e35",
   "metadata": {},
   "source": [
    "## Efficient Testing Strategy\n",
    "\n",
    "How we can test efficiently:\n",
    "\n",
    "- Testing hit rate (H) is fast and relatively inexpensive\n",
    "- While testing H, we can estimate cost (C) by counting returned tokens\n",
    "- Let's collect 25-50 different combinations of size, step, and top_k\n",
    "- Compute the H and token count parts of our objective function\n",
    "- Select 6-8 top candidates\n",
    "- Run full agent evaluation on those candidates to get relevance (R)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd6c12d",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772b3f0a",
   "metadata": {},
   "source": [
    "Set up our evaluation framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988d966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from minsearch import Index\n",
    "import docs\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5fc2bc",
   "metadata": {},
   "source": [
    "Load the ground truth data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_ground_truth = pd.read_csv('../evals/ground_truth_evidently.csv')\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90232eac",
   "metadata": {},
   "source": [
    "Load the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a043ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_data = docs.read_github_data()\n",
    "parsed_data = docs.parse_data(github_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa93f21",
   "metadata": {},
   "source": [
    "Calculate the number of tokens (run uv add tiktoken to install [tiktoken](https://github.com/openai/tiktoken)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277ae1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "def calculate_num_tokens(search_results):\n",
    "    json_result = json.dumps(search_results)\n",
    "    num_tokens = len(encoding.encode(json_result))\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ae6df0",
   "metadata": {},
   "source": [
    "We'll use the functions for calculating hit_rate and mrr from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52cd88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "                break\n",
    "\n",
    "    return total_score / len(relevance_total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76ae24",
   "metadata": {},
   "source": [
    "Put everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae42b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "        ground_truth,\n",
    "        search_function,\n",
    "        question_column='question',\n",
    "        id_column='filename'\n",
    "):\n",
    "    relevance_total = []\n",
    "    tokens = []\n",
    "\n",
    "    for q in ground_truth:\n",
    "        doc_id = q[id_column]\n",
    "        results = search_function(q[question_column])\n",
    "        num_tokens = calculate_num_tokens(results)\n",
    "        tokens.append(num_tokens)\n",
    "        relevance = [d[id_column] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    avg_tokens = sum(tokens) / len(tokens)\n",
    "    \n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "        'num_tokens': avg_tokens\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462049ea",
   "metadata": {},
   "source": [
    "Create the chunking evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e1f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chunks(size, step, top_k):\n",
    "    chunks = docs.chunk_documents(parsed_data, size=size, step=step)\n",
    "    \n",
    "    index = Index(\n",
    "        text_fields=[\"content\", \"filename\", \"title\", \"description\"],\n",
    "    )\n",
    "    \n",
    "    index.fit(chunks)\n",
    "    \n",
    "    def search(query: str):\n",
    "        return index.search(\n",
    "            query=query,\n",
    "            num_results=top_k,\n",
    "        )\n",
    "\n",
    "    return evaluate(ground_truth, search)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221275e2",
   "metadata": {},
   "source": [
    "## Running Systematic Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c0c53",
   "metadata": {},
   "source": [
    "Execute our parameter sweep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ebbaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [1000, 2000, 3000, 5000]\n",
    "steps = [1000, 2000, 3000]\n",
    "top_ks = [5, 10, 15]\n",
    "\n",
    "results = []\n",
    "\n",
    "for size in sizes:\n",
    "    for step in steps:\n",
    "        if step > size: \n",
    "            continue\n",
    "\n",
    "        for top_k in top_ks:\n",
    "            print(f\"{size=}, {step=}, {top_k=}\")\n",
    "            result = evaluate_chunks(size, step, top_k)\n",
    "            print(result)\n",
    "            results.append((size, step, top_k, result['hit_rate'], result['num_tokens']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8515760",
   "metadata": {},
   "source": [
    "## Analyzing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee886f82",
   "metadata": {},
   "source": [
    "Process and score the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a3d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results, columns=['size', 'step', 'top_k', 'hit_rate', 'num_tokens'])\n",
    "\n",
    "alpha = 2\n",
    "beta = 0.5\n",
    "df['score'] = (df.hit_rate ** alpha) / ((df.num_tokens / 1000) ** beta)\n",
    "\n",
    "df = df.sort_values(by='score', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd020cc",
   "metadata": {},
   "source": [
    "This is our scoring function:\n",
    "\n",
    "```text\n",
    "            hit_rate ** alpha\n",
    "score = ------------------------\n",
    "        (num_tokens/1000) ** beta\n",
    "```\n",
    "\n",
    "This represents \"retrieval quality adjusted for the cost of processing tokens\".\n",
    "\n",
    "`The numerator (hit_rate) ** 2` rewards retrieval quality non-linearly (with alpha = 2). Doubling hit rate is much more valuable than doubling cost savings. Squaring makes the metric more sensitive to improvements in hit rate.\n",
    "\n",
    "In other words, we really value configurations that retrieve more correct chunks.\n",
    "\n",
    "`The denominator (num_tokens) ** 0.5` penalizes cost but softer than linearly. Doubling the token cost only reduces the score by ~1.4 (with beta = 0.5).\n",
    "\n",
    "This means we still care about efficiency, but we don't let it dominate quality.\n",
    "\n",
    "You can play with the parameters α and β to see which values rise to the top and adjust accordingly.\n",
    "\n",
    "Look at the top performing configurations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3d5e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df66c6",
   "metadata": {},
   "source": [
    "## Testing Top Candidates with Full Agent Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b19a1e1",
   "metadata": {},
   "source": [
    "Run full evaluations to get relevance scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09495bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = df[:5].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb2c62f",
   "metadata": {},
   "source": [
    "Next, we need to ajust the code to make it possible to configure our agent.\n",
    "\n",
    "First, create AgentConfig class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acffb825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentConfig:\n",
    "    model_name: str = \"openai:gpt-4o-mini\"\n",
    "\n",
    "    chunk_size: int = 2000\n",
    "    chunk_step: int = 1000\n",
    "    top_k: int = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f03ebb",
   "metadata": {},
   "source": [
    "Next, update all the method signatures to accept this config:\n",
    "\n",
    "- run_full_evaluation - add Agent parameter\n",
    "- prepare_search_tools - add chunk size, step and top_k parameter\n",
    "- SearchTools - add top_k parameter\n",
    "- [See full diff here](https://github.com/alexeygrigorev/ai-bootcamp-codespace/commit/e88074ed8a999d00be5a92add3bd0486920b31f0)\n",
    "\n",
    "Now let's run the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cdebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import search_agent\n",
    "\n",
    "param_set = params[0]\n",
    "\n",
    "config = search_agent.AgentConfig(\n",
    "    chunk_size=param_set['size'],\n",
    "    chunk_step=param_set['step'],\n",
    "    top_k=param_set['top_k']\n",
    ")\n",
    "\n",
    "agent = search_agent.create_agent(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e4af4",
   "metadata": {},
   "source": [
    "Run the complete evaluation pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a8a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.eval_orchestrator import run_full_evaluation\n",
    "\n",
    "await run_full_evaluation(agent, csv_path='../evals/gt-sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f91624b",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "```text\n",
    "Evaluation Metrics:\n",
    "\n",
    "  ✓ CheckName.tool_call_search    92.3%\n",
    "  ✓ CheckName.instructions_follow 100.0%\n",
    "  ✓ CheckName.instructions_avoid  100.0%\n",
    "  ⚠ CheckName.answer_relevant     75.0%\n",
    "  ⚠ CheckName.answer_clear        71.4%\n",
    "  ⚠ CheckName.answer_match        71.4%\n",
    "  ⚠ CheckName.answer_citations    71.4%\n",
    "  ⚠ CheckName.completeness        71.4%\n",
    "```\n",
    "\n",
    "Now you can do this for all the top parameter sets and select the one with highest relevance (adjusted by cost)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7f4eaa",
   "metadata": {},
   "source": [
    "## Final Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff485c5",
   "metadata": {},
   "source": [
    "At the end, relevance (R) matters more than hit rate (H) for the end user experience. So we can simplify our final scoring to:\n",
    "\n",
    "Score = R^α / C^β\n",
    "\n",
    "Use this score to select the best approach for your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e7608",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1218c899",
   "metadata": {},
   "source": [
    "Now we can use a data-driven approach for selecting the best chunking parameters. In our case, we can find the optimal balance between quality and cost for our specific application.\n",
    "\n",
    "The framework is reusable - whenever you change your data, model, or requirements, you can re-run this analysis to find the new optimal parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
