{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1664da9e",
   "metadata": {},
   "source": [
    "# Judge Evaluations with Evidently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fbfa96",
   "metadata": {},
   "source": [
    "Evidently is an open-source Python library for evaluating, testing, and monitoring ML models and LLM applications. It provides tools to assess data quality, detect drift, and evaluate LLM outputs using techniques like LLM-as-a-Judge. Evidently generates interactive reports and dashboards to help you understand model performance and behavior in production.\n",
    "\n",
    "This lesson demonstrates how to evaluate LLM responses using Evidently's LLM-as-a-Judge capabilities. You'll learn to compare generated answers against reference answers and create evaluation reports.\n",
    "\n",
    "This is based on [one of the lessons from their course](https://github.com/evidentlyai/community-examples/blob/main/learn/LLMCourse_Tutorial_1_Intro_to_LLM_evals_methods.ipynb).\n",
    "\n",
    "First, install evidently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv add evidently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49242fd6",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9208f",
   "metadata": {},
   "source": [
    "Load the GitHub documentation data that will serve as our reference answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs\n",
    "\n",
    "github_data = docs.read_github_data()\n",
    "parsed_data = docs.parse_data(github_data)\n",
    "\n",
    "file_index = {d['filename']: d['content'] for d in parsed_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df0610",
   "metadata": {},
   "source": [
    "Read the run results from the previous iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('sample_eval_rows.bin', 'rb') as f_in:\n",
    "    rows = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9318d176",
   "metadata": {},
   "source": [
    "Create a DataFrame with evaluation rows and map each question to its reference content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_evals = pd.DataFrame(rows)\n",
    "\n",
    "df_evals['filename'] = df_evals.original_question.apply(lambda x: x['filename'])\n",
    "df_evals['reference'] = df_evals.filename.apply(file_index.get)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a03fba",
   "metadata": {},
   "source": [
    "## Configure LLM Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884bcfb7",
   "metadata": {},
   "source": [
    "Import the necessary Evidently components for LLM evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc9ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently import Dataset, DataDefinition\n",
    "from evidently.descriptors import LLMEval\n",
    "from evidently.llm.templates import MulticlassClassificationPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f073a3a",
   "metadata": {},
   "source": [
    "Define the evaluation criteria using a multiclass classification template. This template instructs the LLM judge to categorize answers into match, partial_match, mismatch, or not_available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6af57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = MulticlassClassificationPromptTemplate(\n",
    "    pre_messages=[\n",
    "        (\"system\", \"You are a judge that evaluates the factual alignment of two chatbot answers.\")\n",
    "    ],\n",
    "    criteria=\"\"\"\n",
    "    You are given a question, a new answer and a reference answer. \n",
    "    Classify the new answer based on how it compares to the reference.\n",
    "    ===\n",
    "    Question: {question}\n",
    "    Reference: {reference}\n",
    "    \"\"\",\n",
    "    category_criteria={\n",
    "        \"match\": \"The answer matches the reference in all factual and semantic details.\",\n",
    "        \"partial_match\": \"The answer is correct in what it says but leaves out details from the reference.\",\n",
    "        \"mismatch\": \"The answer doesn't match the reference answer.\",\n",
    "        \"not_available\": \"The answer says that information is not available.\",\n",
    "    },\n",
    "    uncertainty=\"unknown\",\n",
    "    include_reasoning=True,\n",
    "    include_scores=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d798263",
   "metadata": {},
   "source": [
    "## Create Evaluation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256e08da",
   "metadata": {},
   "source": [
    "Create an Evidently Dataset with LLM evaluation descriptor. This will evaluate each answer against its reference using \"gpt-4o-mini\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bfb4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = Dataset.from_pandas(\n",
    "    data=df_evals,\n",
    "    data_definition=DataDefinition(),\n",
    "    descriptors=[\n",
    "        LLMEval(\n",
    "            column_name=\"answer\",\n",
    "            additional_columns={\"question\": \"question\", \"reference\": \"reference\"},\n",
    "            template=matcher,\n",
    "            provider=\"openai\",\n",
    "            model=\"gpt-4o-mini\",\n",
    "            alias=\"eval\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c611c",
   "metadata": {},
   "source": [
    "Convert the evaluated dataset back to a DataFrame for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fdacab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_result = eval_dataset.as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aebcb15",
   "metadata": {},
   "source": [
    "## Analyze Evaluation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf2b097",
   "metadata": {},
   "source": [
    "Examine the reasoning provided by the LLM judge for a specific evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f877306",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_result.iloc[1]['eval reasoning']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6651e3",
   "metadata": {},
   "source": [
    "View the original question that was evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382df2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_result.iloc[1]['question']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb1c34",
   "metadata": {},
   "source": [
    "View the answer that was evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e3cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_result.iloc[1]['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb43f0",
   "metadata": {},
   "source": [
    "## Generate Evaluation Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca79a5d",
   "metadata": {},
   "source": [
    "We can now calculate some statistics from the dataframe. But we can also use Evidently's built-in reporting.\n",
    "\n",
    "Import the reporting components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3114cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evidently import Report\n",
    "from evidently.presets import TextEvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf700d8a",
   "metadata": {},
   "source": [
    "Create and run a comprehensive text evaluation report using the TextEvals preset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568527fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report([\n",
    "    TextEvals()\n",
    "])\n",
    "\n",
    "my_eval = report.run(eval_dataset, None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e9e4b",
   "metadata": {},
   "source": [
    "Display the evaluation report with metrics and visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede67fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53552bcc",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea91f6a",
   "metadata": {},
   "source": [
    "With Evidently it's quite easy to run evals and then display the reports"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
