{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a095739c",
   "metadata": {},
   "source": [
    "# LangWatch Scenario for Agent Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3565a66c",
   "metadata": {},
   "source": [
    "So far we used plain pytest for testing and also implemented a judge for that.\n",
    "\n",
    "It's good, but there are also specialized tools for agent testing.\n",
    "\n",
    "One of them is [LangWatch Scenario](https://github.com/langwatch/scenario). You can see their [demo here](https://www.youtube.com/watch?v=OHg02uRg5kE).\n",
    "\n",
    "In this lesson I want to show you a simple case of using Scenario. If you like it, you can explore it more. I also find the implementation interesting - there are things to learn from reading the code.\n",
    "\n",
    "It uses agents to test agents, which provides valuable insights into how these testing frameworks work internally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5344019",
   "metadata": {},
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3400545",
   "metadata": {},
   "source": [
    "Install Scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv add --dev langwatch-scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c2ead",
   "metadata": {},
   "source": [
    "Create a new test file: `_test_agent_scenario.py` (`tests/_test_agent_scenario.py`)\n",
    "\n",
    "I put an underscore in front so it doesn't get picked up by default pytest discovery mechanisms.\n",
    "\n",
    "This allows us to run these tests separately when we want to experiment with Scenario without affecting our regular test suite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591fe00c",
   "metadata": {},
   "source": [
    "## Creating the Agent Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dea0336",
   "metadata": {},
   "source": [
    "The code in this lesson is based on this example: [examples/lovable_clone/lovable_agent.py](https://github.com/langwatch/scenario/blob/main/python/examples/lovable_clone/lovable_agent.py). It also uses Pydantic AI, so I decided to adapt it.\n",
    "\n",
    "Since we use Pydantic AI, we need to create a wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921eb902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scenario\n",
    "\n",
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "\n",
    "from main import run_agent\n",
    "\n",
    "class SearchAgentAdapter(scenario.AgentAdapter):\n",
    "\n",
    "    async def call(self, input: scenario.AgentInput) -> scenario.AgentReturnTypes:\n",
    "        user_prompt = input.last_new_user_message_str()\n",
    "        result = await run_agent(user_prompt)\n",
    "        new_messages = result.new_messages()\n",
    "        return await self.convert_to_openai_format(new_messages)\n",
    "\n",
    "    async def convert_to_openai_format(self, messages):\n",
    "        openai_model = OpenAIChatModel(\"any\")\n",
    "        new_messages_openai_format = []\n",
    "        for openai_message in await openai_model._map_messages(messages):\n",
    "            new_messages_openai_format.append(openai_message)\n",
    "\n",
    "        return new_messages_openai_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a67a32",
   "metadata": {},
   "source": [
    "This adapter bridges between our Pydantic AI agent and Scenario's expected interface.\n",
    "\n",
    "The call method extracts the user's message and runs our agent. The `convert_to_openai_format` method transforms Pydantic AI's message format back to OpenAI's format, which Scenario expects for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8f712",
   "metadata": {},
   "source": [
    "## Defining a Test Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae364c",
   "metadata": {},
   "source": [
    "Now we define a scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a9e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.mark.asyncio\n",
    "async def test_agent_code():\n",
    "    result = await scenario.run(\n",
    "        name=\"Evidentily Documentation\",\n",
    "        description=\"\"\"\n",
    "            The agent is tasked with asking questions about 'LLM as a Judge' evaluation.\n",
    "            Send the first message to ask the question and then follow up\n",
    "            with another question to understand the topic better. \n",
    "        \"\"\",\n",
    "        agents=[\n",
    "            SearchAgentAdapter(),\n",
    "            scenario.UserSimulatorAgent(),\n",
    "            scenario.JudgeAgent(\n",
    "                criteria=[\n",
    "                    \"agent makes 3 search calls\",\n",
    "                    \"the references are relevant to the topic\",\n",
    "                    \"each section has references\",\n",
    "                    \"the article contains properly formatted python code examples\"\n",
    "                ],\n",
    "            ),\n",
    "        ],\n",
    "        max_turns=2,\n",
    "        set_id=\"python-example\",\n",
    "    )\n",
    "\n",
    "    assert result.success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b308737",
   "metadata": {},
   "source": [
    "Here the important parts are:\n",
    "\n",
    "description guides the UserSimulatorAgent on how to interact with our search agent. It acts as a testing scenario specification.\n",
    "\n",
    "JudgeAgent analyzes the entire interaction and checks if all criteria are satisfied. It's an LLM-based evaluator that can understand complex requirements.\n",
    "\n",
    "JudgeCriteria.criteria are the specific requirements we want to verify. The JudgeAgent evaluates whether our agent meets these criteria.\n",
    "\n",
    "UserSimulatorAgent pretends to be a user and uses the description to interact with our agent in a realistic way.\n",
    "\n",
    "Like previously, we use LLM as a Judge. It can evaluate nuanced behaviors that would be difficult to check with traditional assertions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c823c611",
   "metadata": {},
   "source": [
    "## Running the Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042144b6",
   "metadata": {},
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2638df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run pytest tests/_test_agent_scenario.py::test_agent_code -s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cef8f9",
   "metadata": {},
   "source": [
    "We see a nice output:\n",
    "\n",
    "\n",
    "```text\n",
    "Total Scenarios: 1\n",
    "Passed: 1\n",
    "Failed: 0\n",
    "Success Rate: 100.0%\n",
    "\n",
    "1. Evidentily Documentation - PASSED\n",
    "   Reasoning: The agent successfully made three search calls, gathered relevant information about LLM as a judge evaluation, and provided references for each section. Additionally, the content includes properly formatted Python code examples related to the evaluation process.\n",
    "   Passed Criteria: 4/4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31aea54",
   "metadata": {},
   "source": [
    "## When to Use Scenario vs Traditional Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d553f",
   "metadata": {},
   "source": [
    "Use Scenario when:\n",
    "\n",
    "- You need complex multi-turn conversations\n",
    "- You want to test realistic user interactions\n",
    "- You need sophisticated evaluation criteria that are hard to code\n",
    "- You want to simulate different user behaviors\n",
    "\n",
    "Use traditional tests when:\n",
    "\n",
    "- You need simpler unit tests\n",
    "- You need to test specific functions or components\n",
    "- You want fine-grained control over test execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5a4f42",
   "metadata": {},
   "source": [
    "## Code from Video..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1697622f",
   "metadata": {},
   "source": [
    "`tests/_test_agent_scenario.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a4e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import scenario\n",
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "import main\n",
    "\n",
    "scenario.configure(default_model=\"openai/gpt-4o-mini\")\n",
    "\n",
    "class SearchAgentAdapter(scenario.AgentAdapter):\n",
    "\n",
    "    async def call(self, input: scenario.AgentInput) -> scenario.AgentReturnTypes:\n",
    "        user_prompt = input.last_new_user_message_str()\n",
    "        result = await main.run_agent(user_prompt)\n",
    "        new_messages = result.new_messages()\n",
    "        return await self.convert_to_openai_format(new_messages)\n",
    "\n",
    "    async def convert_to_openai_format(self, messages):\n",
    "        openai_model = OpenAIChatModel(\"any\")\n",
    "        new_messages_openai_format = []\n",
    "        for openai_message in await openai_model._map_messages(messages):\n",
    "            new_messages_openai_format.append(openai_message)\n",
    "\n",
    "        return new_messages_openai_format\n",
    "    \n",
    "@pytest.mark.asyncio\n",
    "async def test_agent_code():\n",
    "\n",
    "    user_prompt = \"How do I implement LLM as a Judge eval?\"\n",
    "\n",
    "    result = await scenario.run(\n",
    "        name=\"Evidently Search Agent Code Test\",\n",
    "        description=\"\"\"\n",
    "            User asks for help with implementing LLM as a Judge evaluation in Evidently\n",
    "        \"\"\",\n",
    "        agents=[\n",
    "            SearchAgentAdapter(),\n",
    "            scenario.UserSimulatorAgent(),\n",
    "            scenario.JudgeAgent(\n",
    "                criteria=[\n",
    "                    \"Provides accurate and relevant code examples\",\n",
    "                    \"Explains code implementation clearly\",\n",
    "                    \"Contains at least one python code block in the article\",\n",
    "                    \"Contains references\"\n",
    "                ],\n",
    "            ),\n",
    "        ],\n",
    "        max_turns=2,\n",
    "        set_id=\"python-example\",\n",
    "    )\n",
    "\n",
    "    assert result.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea773f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run pytest tests/_test_agent_scenario.py::test_agent_code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
