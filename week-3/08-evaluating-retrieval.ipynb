{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbddb78",
   "metadata": {},
   "source": [
    "# Evaluating Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db2d09f",
   "metadata": {},
   "source": [
    "We'll evaluate different search approaches using our synthetic dataset. We'll compare keyword-based search, vector search, and hybrid approaches to see which performs best.\n",
    "\n",
    "This systematic evaluation will help us understand which search methods work better for different types of queries and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f09e851",
   "metadata": {},
   "source": [
    "## Setting Up the Search System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa3147",
   "metadata": {},
   "source": [
    "First, let's set up our document index and search function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c2eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs\n",
    "\n",
    "github_data = docs.read_github_data()\n",
    "parsed_data = docs.parse_data(github_data)\n",
    "chunks = docs.chunk_documents(parsed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baccf98c",
   "metadata": {},
   "source": [
    "Unlike the previous lesson, here we chunk the documents into smaller pieces. This allows us to match specific sections rather than entire documents.\n",
    "\n",
    "Let's index the chunked documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b036fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index\n",
    "from typing import Any, Dict, List, TypedDict\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"content\", \"filename\", \"title\", \"description\"],\n",
    ")\n",
    "\n",
    "index.fit(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c72475",
   "metadata": {},
   "source": [
    "Now we define our baseline search function using keyword-based search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d32545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchResult(TypedDict):\n",
    "    \"\"\"Represents a single search result entry.\"\"\"\n",
    "    start: int\n",
    "    content: str\n",
    "    title: str\n",
    "    description: str\n",
    "    filename: str\n",
    "\n",
    "\n",
    "def search(query: str) -> List[SearchResult]:\n",
    "    \"\"\"\n",
    "    Search the index for documents matching the given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[SearchResult]: A list of search results. Each result dictionary contains:\n",
    "            - start (int): The starting position or offset within the source file.\n",
    "            - content (str): A text excerpt or snippet containing the match.\n",
    "            - title (str): The title of the matched document.\n",
    "            - description (str): A short description of the document.\n",
    "            - filename (str): The path or name of the source file.\n",
    "    \"\"\"\n",
    "    return index.search(\n",
    "        query=query,\n",
    "        num_results=5,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950961fe",
   "metadata": {},
   "source": [
    "## Loading Ground Truth Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b5611",
   "metadata": {},
   "source": [
    "We need to load the synthetic dataset we created earlier. This will serve as our evaluation benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ee5c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_ground_truth = pd.read_csv('ground_truth_evidently.csv')\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b4a23",
   "metadata": {},
   "source": [
    "The ground truth data contains questions paired with their expected source documents, which allows us to measure search accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337bc6e4",
   "metadata": {},
   "source": [
    "## Collecting Search Results for Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51f199c",
   "metadata": {},
   "source": [
    "Now let's run our search function against all questions in the ground truth dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "all_search_results = []\n",
    "\n",
    "for gt_rec in tqdm(ground_truth):\n",
    "    sr = search(gt_rec['question'])\n",
    "    filename = gt_rec['filename']\n",
    "    relevance = [filename == sr_rec['filename'] for sr_rec in sr]\n",
    "    all_search_results.append(relevance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525f023",
   "metadata": {},
   "source": [
    "This creates a list of relevance scores for each query. Each inner list shows whether the search results are relevant (True) or not (False).\n",
    "\n",
    "This is how the relevance data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c594924",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[True, True, True, True, True],\n",
    " [False, False, False, False, False],\n",
    " [False, False, False, False, False],\n",
    " [False, False, False, False, True],\n",
    " [False, False, False, False, False],\n",
    " [False, False, False, False, False],\n",
    " [False, False, False, False, False],\n",
    " [False, False, False, False, False],\n",
    " [False, False, False, False, False],\n",
    " [False, False, False, False, False]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e98cc2a",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ff9e2",
   "metadata": {},
   "source": [
    "We will implement two search evaluation metrics:\n",
    "\n",
    "- **Hit Rate**: The percentage of queries where at least one relevant document appears in the top results.\n",
    "\n",
    "- **Mean Reciprocal Rank (MRR)**: The average of reciprocal ranks of the first relevant document. It rewards finding relevant documents early in the result list.\n",
    "\n",
    "Here's how to calculate them with an example.\n",
    "\n",
    "With hitrate we only care if we managed to hit the document with the right id in the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797aa9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [\n",
    "    [True, False, False, False, False],  # 1\n",
    "    [False, False, False, False, False], # 0\n",
    "    [False, False, False, False, False], # 0\n",
    "    [False, False, False, False, False], # 0\n",
    "    [False, False, False, False, False], # 0\n",
    "    [True, False, False, False, False],  # 1\n",
    "    [False, True, False, False, False],  # 1\n",
    "    [False, False, True, False, False],  # 1\n",
    "    [False, False, False, True, False],  # 1\n",
    "    [True, False, False, False, False],  # 1\n",
    "    [False, False, True, False, False],  # 1\n",
    "    [False, False, False, False, False], # 0\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1694557",
   "metadata": {},
   "source": [
    "Hitrate here is 7/10.\n",
    "\n",
    "Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00988b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51795c6e",
   "metadata": {},
   "source": [
    "For MRR, we also look at the position:\n",
    "\n",
    "- if the relevant result is at position 1, score is 1\n",
    "- position 2 => 1/2\n",
    "- position 3 => 1/3\n",
    "- position 4 => 1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8879cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [\n",
    "    # 1      2       3      4      5\n",
    "    [True, False, False, False, False],  # 1\n",
    "    [False, False, False, False, False], # 0\n",
    "    [False, False, False, False, False], # 0\n",
    "    [False, False, False, False, False], # 0\n",
    "    [False, False, False, False, False], # 0\n",
    "    [True, False, False, False, False],  # 1\n",
    "    [False, True, False, False, False],  # 1/2\n",
    "    [False, False, True, False, False],  # 1/3\n",
    "    [False, False, False, True, False],  # 1/4\n",
    "    [True, False, False, False, False],  # 1\n",
    "    [False, False, True, False, False],  # 1/3\n",
    "    [False, False, False, False, False], # 0\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8099b7a",
   "metadata": {},
   "source": [
    "Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "                break\n",
    "\n",
    "    return total_score / len(relevance_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4126f8",
   "metadata": {},
   "source": [
    "We can put them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c7823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "        ground_truth,\n",
    "        search_function,\n",
    "        question_column='question',\n",
    "        id_column='filename'\n",
    "):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q[id_column]\n",
    "        results = search_function(q[question_column])\n",
    "        relevance = [d[id_column] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36799333",
   "metadata": {},
   "source": [
    "## Baseline Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25cc480",
   "metadata": {},
   "source": [
    "Let's evaluate our keyword-based search to establish a baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(ground_truth, search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4285fdf7",
   "metadata": {},
   "source": [
    "The baseline performance shows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7fd1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'hit_rate': 0.4386694386694387, 'mrr': 0.3626472626472625}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff16eef",
   "metadata": {},
   "source": [
    "This means our keyword search finds relevant documents in only 44% of queries, with an average reciprocal rank of 36%. This isn't great, but it gives us a starting point. The synthetic data we generated is quite challenging, which makes it a good benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680379ad",
   "metadata": {},
   "source": [
    "## Vector Search Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa60adde",
   "metadata": {},
   "source": [
    "Keyword search struggles with semantic similarity. Let's try vector search, which can understand the meaning behind queries.\n",
    "\n",
    "We need to add `sentence-transformer`\n",
    "\n",
    "```bash\n",
    "uv add sentence-transformers\n",
    "```\n",
    "\n",
    "First, we need to set up the embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6fbf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db36691b",
   "metadata": {},
   "source": [
    "Create embeddings for all document chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "embeddings = []\n",
    "\n",
    "for d in tqdm(chunks):\n",
    "    text = d.get('title', '') + ' ' + d.get('description', '') + ' ' + d.get('content', '')\n",
    "    text = text.strip()\n",
    "    v = embedding_model.encode(text)\n",
    "    embeddings.append(v)\n",
    "\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf0aeaa",
   "metadata": {},
   "source": [
    "And index them with vector search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea17e517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import VectorSearch\n",
    "\n",
    "vindex = VectorSearch()\n",
    "vindex.fit(embeddings, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835bb179",
   "metadata": {},
   "source": [
    "Define the vector search function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e8319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_search(query: str) -> List[SearchResult]:\n",
    "    \"\"\"\n",
    "    Search the index for documents matching the given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[SearchResult]: A list of search results. Each result dictionary contains:\n",
    "            - start (int): The starting position or offset within the source file.\n",
    "            - content (str): A text excerpt or snippet containing the match.\n",
    "            - title (str): The title of the matched document.\n",
    "            - description (str): A short description of the document.\n",
    "            - filename (str): The path or name of the source file.\n",
    "    \"\"\"\n",
    "\n",
    "    q = embedding_model.encode(query)\n",
    "\n",
    "    return vindex.search(\n",
    "        q,\n",
    "        num_results=5,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbccfd18",
   "metadata": {},
   "source": [
    "## Vector Search Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3423e5",
   "metadata": {},
   "source": [
    "Let's see how vector search performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(ground_truth, v_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be079351",
   "metadata": {},
   "source": [
    "The results show significant improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d71648",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'hit_rate': 0.762993762993763, 'mrr': 0.5922383922383921}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55648ab",
   "metadata": {},
   "source": [
    "Vector search achieves 76% hit rate and 59% MRR - much better than keyword search. This demonstrates the power of semantic understanding over exact keyword matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797dfb1",
   "metadata": {},
   "source": [
    "## Hybrid Search Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d08483",
   "metadata": {},
   "source": [
    "Can we get even better results by combining both approaches? Let's try a hybrid search that uses both vector and keyword search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b7f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_search(query: str) -> List[SearchResult]:\n",
    "    \"\"\"\n",
    "    Search the index for documents matching the given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[SearchResult]: A list of search results. Each result dictionary contains:\n",
    "            - start (int): The starting position or offset within the source file.\n",
    "            - content (str): A text excerpt or snippet containing the match.\n",
    "            - title (str): The title of the matched document.\n",
    "            - description (str): A short description of the document.\n",
    "            - filename (str): The path or name of the source file.\n",
    "    \"\"\"\n",
    "\n",
    "    return v_search(query) + search(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ad6f6",
   "metadata": {},
   "source": [
    "## Hybrid Search Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a3a5be",
   "metadata": {},
   "source": [
    "Let's evaluate the hybrid approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca13385",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(ground_truth, h_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7723a95c",
   "metadata": {},
   "source": [
    "The hybrid search shows the best performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be6e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'hit_rate': 0.8045738045738046, 'mrr': 0.5981569481569481}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e8e34",
   "metadata": {},
   "source": [
    "With an 80% hit rate, the hybrid approach finds relevant documents for 4 out of 5 queries. The MRR also improved slightly, showing better ranking quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4892a7fc",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fce1ef3",
   "metadata": {},
   "source": [
    "Our systematic evaluation reveals clear performance differences between search approaches:\n",
    "\n",
    "- Keyword Search: 44% hit rate\n",
    "- Vector Search: 76% hit rate\n",
    "- Hybrid Search: 80% hit rate\n",
    "\n",
    "The synthetic dataset is very useful for this evaluation. It provided a large set of queries that allowed us to quantify search quality objectively and select the best search method for our search system.\n",
    "\n",
    "But we can also use it for evaluating our agent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
