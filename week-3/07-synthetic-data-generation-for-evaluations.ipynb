{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6d556c",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation for Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778fc3e5",
   "metadata": {},
   "source": [
    "We'll create a system to automatically generate realistic user questions based on the technical documentation usecase.\n",
    "\n",
    "This synthetic data can be used for testing our agent and its search function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286cf5a2",
   "metadata": {},
   "source": [
    "```bash\n",
    "uv add --dev jupyter\n",
    "uv run jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79800d15",
   "metadata": {},
   "source": [
    "## Loading and Processing Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b564485",
   "metadata": {},
   "source": [
    "First, let's load the documentation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs\n",
    "\n",
    "raw_documents = docs.read_github_data()\n",
    "documents = docs.parse_data(raw_documents)\n",
    "len(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1895a553",
   "metadata": {},
   "source": [
    "Note that we don't do chunking here. We want to use the complete documents for generating the data.\n",
    "\n",
    "If for your case the data is too big (e.g. it's a book), then use some logical-based chunking - e.g. per chapter/section, and then use them for creating questions.\n",
    "\n",
    "In this case, we get 95 documents total. Now let's filter and select documents that are suitable for question generation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736c7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_questions_total = 0\n",
    "\n",
    "selected_documents = []\n",
    "\n",
    "for doc in documents[5:]:\n",
    "    if 'title' not in doc:\n",
    "        continue\n",
    "\n",
    "    title = doc['title']\n",
    "    if 'unpublished' in title.lower():\n",
    "        continue\n",
    "    if 'legacy' in title.lower():\n",
    "        continue\n",
    "    if 'leftovers' in title.lower():\n",
    "        continue\n",
    "\n",
    "    content = doc.get('content', '').strip()\n",
    "    if len(content) <= 1000:\n",
    "        continue\n",
    "\n",
    "    # We want to create one question for every 1000 characters\n",
    "    num_questions = len(content) // 1000\n",
    "    print(doc.get('title'))\n",
    "    print(len(content), num_questions)\n",
    "    num_questions_total = num_questions_total + num_questions\n",
    "    print('------------')\n",
    "\n",
    "    selected_documents.append(doc)\n",
    "\n",
    "print(num_questions_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71a4b52",
   "metadata": {},
   "source": [
    "This filtering process:\n",
    "\n",
    "Skips documents without titles\n",
    "\n",
    "Excludes unpublished, legacy, and leftover content\n",
    "\n",
    "Only includes substantial documents (over 1000 characters)\n",
    "\n",
    "The longer the document, the more questions we may want to generate. So let's use the following logic: for each 1000 characters of the document we generate one question.\n",
    "\n",
    "This way we can generate approximately 471 questions from 69 selected documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a1860",
   "metadata": {},
   "source": [
    "## Setting Up the LLM Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47387a4",
   "metadata": {},
   "source": [
    "We already know how to produce structured output. For this lesson let's use the OpenAI client. We will re-use the helper function for structured output that we created in Week 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a06dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def llm_structured(instructions, user_prompt, output_format, model=\"gpt-4o-mini\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.parse(\n",
    "        model=model,\n",
    "        input=messages,\n",
    "        text_format=output_format\n",
    "    )\n",
    "\n",
    "    return (response.output_parsed, response.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9528d26b",
   "metadata": {},
   "source": [
    "# Developing Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b2ebd",
   "metadata": {},
   "source": [
    "Let's start with these instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_instructions = \"\"\"\n",
    "you're given an article and your task is to think what kind of questions \n",
    "the reader of this article may have\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf5d46",
   "metadata": {},
   "source": [
    "And very simple schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1feacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "    summary_answer: str\n",
    "\n",
    "class GeneratedQuestions(BaseModel):\n",
    "    questions: list[Question]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51899d09",
   "metadata": {},
   "source": [
    "Run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def process_document(doc):\n",
    "    content = doc['content']\n",
    "    num_questions = len(content) // 1000\n",
    "    user_prompt = f\"\"\"generate {num_questions} for this document:\n",
    "    <document>{json.dumps(doc)}</document>\n",
    "    \"\"\"\n",
    "    response, usage = llm_structured(\n",
    "        instructions=generator_instructions,\n",
    "        user_prompt=user_prompt,\n",
    "        output_format=GeneratedQuestions\n",
    "    )\n",
    "    return {\n",
    "        'doc': doc,\n",
    "        'questions': response.questions,\n",
    "        'usage': usage\n",
    "    }\n",
    "\n",
    "doc = selected_documents[0]\n",
    "result = process_document(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ea246",
   "metadata": {},
   "source": [
    "The results don't look like search queries yet. Let's ask it explicitly:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3615ec4e",
   "metadata": {},
   "source": [
    "`formulate it as a search query`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e386df00",
   "metadata": {},
   "source": [
    "Or with help of ChatGPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c1f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_instructions = \"\"\"\n",
    "You are given a technical article. Your task is to imagine what questions \n",
    "a person might type into a search engine before they find and read this article.\n",
    "\n",
    "Formulate natural, human-like search queries — the way real users might ask them online. \n",
    "Avoid robotic or academic phrasing. Include diversity in tone and style.\n",
    "\n",
    "Assume users have different knowledge levels:\n",
    "- Beginners who are just starting and may not know the terminology.\n",
    "- Intermediate users who understand the basics but need clarification or examples.\n",
    "- Advanced users who seek details, edge cases, or integration insights.\n",
    "\n",
    "Each query should sound plausible as a Google-style search — short, goal-oriented, and reflecting the user’s curiosity or problem.\n",
    "\n",
    "For each generated query:\n",
    "- Provide the likely search question (in natural language).\n",
    "- Provide a summary_answer — a 1–2 sentence explanation summarizing how the article would answer that question.\n",
    "\n",
    "Return results in the GeneratedQuestions schema.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e295d627",
   "metadata": {},
   "source": [
    "The questions are still quite big, so we ask it to \"avoid full-sentence questions with punctuation like 'What is...'\". Plus, we can specify the difficulty and the distribution of questions according to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08276c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_instructions = \"\"\"\n",
    "You are given a technical article. Your task is to imagine what a person might type into a search engine \n",
    "before finding and reading this article.\n",
    "\n",
    "Generate realistic, human-like search queries — not formal questions. \n",
    "They should sound like things real users type into Google or Stack Overflow \n",
    "when trying to solve a problem or learn about the topic.\n",
    "\n",
    "Guidelines:\n",
    "- Avoid full-sentence questions with punctuation like \"What is...\" or \"How do I...\".\n",
    "- Use short, natural search phrases instead, such as:\n",
    "  - \"evidently data definition example\"\n",
    "  - \"map target and prediction columns evidently\"\n",
    "  - \"difference between timestamp and datetime evidently\"\n",
    "- Vary phrasing to sound human and spontaneous.\n",
    "- Assume users of different knowledge levels:\n",
    "  - beginner: broad or basic understanding\n",
    "  - intermediate: knows basic terms but seeks clarification or examples\n",
    "  - advanced: familiar with the tool, looking for specific options or integrations\n",
    "\n",
    "For each generated query:\n",
    "- question: the realistic search phrase\n",
    "- summary_answer: a short, 1–2 sentence summary of how the article answers it\n",
    "- difficulty: one of [\"beginner\", \"intermediate\", \"advanced\"]\n",
    "- 50% of questions should be beginner, 30% intermediate and 20% advanced\n",
    "\n",
    "Also include a description summarizing what kind of article the questions are about.\n",
    "\n",
    "Return results in the GeneratedQuestions schema.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4938501c",
   "metadata": {},
   "source": [
    "Let's also include intent classification to understand whether users are looking for conceptual explanations or code examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc84ff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_instructions = \"\"\"\n",
    "You are given a technical article. Your task is to imagine what a person might type into a search engine \n",
    "before finding and reading this article.\n",
    "\n",
    "Generate realistic, human-like search queries — not formal questions. \n",
    "They should sound like what people actually type into Google or Stack Overflow \n",
    "when trying to solve a problem, learn a concept, or find code examples.\n",
    "\n",
    "Guidelines:\n",
    "- Avoid full-sentence questions with punctuation like \"What is...\" or \"How do I...\".\n",
    "- Use short, natural search phrases instead, such as:\n",
    "  - \"evidently data definition example\"\n",
    "  - \"map target and prediction columns evidently\"\n",
    "  - \"difference between timestamp and datetime evidently\"\n",
    "- Make queries varied and spontaneous, not repetitive or over-polished.\n",
    "- Assume users of different knowledge levels:\n",
    "  - beginner: broad or basic understanding\n",
    "  - intermediate: knows basic terms but seeks clarification or examples\n",
    "  - advanced: familiar with the tool, looking for details, edge cases, or integration options\n",
    "\n",
    "Distribution rules:\n",
    "- 60% of the queries should target beginner-level users\n",
    "- 30% should target intermediate-level users\n",
    "- 10% should target advanced-level users\n",
    "- 75% of queries should have an intent of \"code\" (looking for examples or implementation)\n",
    "- 25% should have an intent of \"text\" (looking for conceptual or theoretical explanations)\n",
    "\n",
    "For each generated query, include:\n",
    "- question: the natural, human-style search phrase\n",
    "- summary_answer: a short 1–2 sentence summary of how the article addresses it\n",
    "- difficulty: one of [\"beginner\", \"intermediate\", \"advanced\"]\n",
    "- intent: one of [\"text\", \"code\"]\n",
    "\n",
    "Also include a description summarizing what kind of article the questions are about.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a2fab4",
   "metadata": {},
   "source": [
    "For the schema, this is the final version with intent classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "\n",
    "class Question(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a realistic search-engine-style query a user might type before finding the article.\n",
    "    Each question captures the likely search phrase, a short summary answer,\n",
    "    the user's assumed skill level, and their intent (conceptual or code-focused).\n",
    "    \"\"\"\n",
    "    question: str = Field(\n",
    "        ...,\n",
    "        description=\"A natural, short search query — not a full-sentence question — phrased like something typed into Google.\"\n",
    "    )\n",
    "    summary_answer: str = Field(\n",
    "        ...,\n",
    "        description=\"A concise 1–2 sentence summary of how the article addresses the query.\"\n",
    "    )\n",
    "    difficulty: Literal[\"beginner\", \"intermediate\", \"advanced\"] = Field(\n",
    "        ...,\n",
    "        description=\"The assumed knowledge level of the user making the query.\"\n",
    "    )\n",
    "    intent: Literal[\"text\", \"code\"] = Field(\n",
    "        ...,\n",
    "        description=\"Specifies if the user's intent is to get a theoretical explanation ('text') or an implementation example ('code').\"\n",
    "    )\n",
    "\n",
    "\n",
    "class GeneratedQuestions(BaseModel):\n",
    "    \"\"\"\n",
    "    A structured collection of human-like search queries derived from a given article.\n",
    "    Includes a brief description of the article topic and a list of generated queries.\n",
    "    Difficulty distribution: 60% beginner, 30% intermediate, 10% advanced.\n",
    "    Intent distribution: 75% code-focused, 25% concept-focused.\n",
    "    \"\"\"\n",
    "    description: str = Field(\n",
    "        ...,\n",
    "        description=\"A summary of the article or topic these search-style questions were generated for.\"\n",
    "    )\n",
    "    questions: List[Question] = Field(\n",
    "        ...,\n",
    "        description=\"A list of realistic search queries with short summaries, difficulty levels, and user intent.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16238a4a",
   "metadata": {},
   "source": [
    "## Processing Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de3461",
   "metadata": {},
   "source": [
    "Let's process all the documents. We want to make it fast, so we'll do this in multiple parallel threads.\n",
    "\n",
    "Here's some helper code for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48a9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def map_progress(pool, seq, f):\n",
    "    \"\"\"Map function f over seq using the provided executor pool while\n",
    "    displaying a tqdm progress bar. Returns a list of results in submission order.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with tqdm(total=len(seq)) as progress:\n",
    "        futures = []\n",
    "    \n",
    "        for el in seq:\n",
    "            future = pool.submit(f, el)\n",
    "            future.add_done_callback(lambda p: progress.update())\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b756f7d",
   "metadata": {},
   "source": [
    "And this is how we use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127b7d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor(max_workers=6) as pool:\n",
    "    all_results = map_progress(pool, selected_documents, process_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4365c405",
   "metadata": {},
   "source": [
    "## Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f9a8f1",
   "metadata": {},
   "source": [
    "Let's see how much it costed to generate the data.\n",
    "\n",
    "Create the pricing config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba25d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.pricing import PricingConfig\n",
    "pricing = PricingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca74db47",
   "metadata": {},
   "source": [
    "Calculate the price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58371f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_input = 0\n",
    "total_output = 0\n",
    "\n",
    "for res in all_results:\n",
    "    usage = res['usage']\n",
    "    total_input = total_input + usage.input_tokens\n",
    "    total_output = total_output + usage.output_tokens\n",
    "\n",
    "pricing.calculate_cost('gpt-4o-mini', total_input, total_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b530c",
   "metadata": {},
   "source": [
    "$0.04\n",
    "\n",
    "Not bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d82563",
   "metadata": {},
   "source": [
    "## Saving the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516647a9",
   "metadata": {},
   "source": [
    "Now let's save all the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions = []\n",
    "\n",
    "for res in all_results:\n",
    "    doc = res['doc']\n",
    "    questions = res['questions']\n",
    "    for q in questions:\n",
    "        q_dict = q.model_dump()\n",
    "        q_dict['filename'] = doc['filename']\n",
    "        all_questions.append(q_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807b20e0",
   "metadata": {},
   "source": [
    "We'll use pandas for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931898f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_questions = pd.DataFrame(all_questions)\n",
    "df_questions.to_csv('ground_truth_evidently.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a8e174",
   "metadata": {},
   "source": [
    "## Summary and next steps\n",
    "\n",
    "We have generated synthetic data for our evaluations. Next we can use it for:\n",
    "\n",
    "- Evaluating search\n",
    "- Evaluating our agent\n",
    "\n",
    "Let's start with search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
