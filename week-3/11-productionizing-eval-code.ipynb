{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc14ca36",
   "metadata": {},
   "source": [
    "# Productionizing Evals Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0798e837",
   "metadata": {},
   "source": [
    "We'll refactor our evaluation code from notebooks into a proper Python project structure. This makes it easy to run evaluations regularly and integrate them into CI/CD pipelines if we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed47fa",
   "metadata": {},
   "source": [
    "## Evaluation Architecture Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b158b287",
   "metadata": {},
   "source": [
    "We want to make running evaluations frictionless. There are two main parts:\n",
    "\n",
    "- Ground truth synthetic data generation (run once)\n",
    "- Evaluations (run regularly)\n",
    "\n",
    "The first part (data generation) is less critical since we run it infrequently. We can keep it in a notebook format.\n",
    "\n",
    "The second part (evaluation) is more important. We want to run it often, so it must be easy to execute.\n",
    "\n",
    "The evaluations part consists of two steps:\n",
    "\n",
    "1. Apply agent to ground truth data to get the output\n",
    "2. Analyze the output to assess the quality\n",
    "\n",
    "To make it easier to execute, we'll move it from a Jupyter notebook into a proper Python project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be024966",
   "metadata": {},
   "source": [
    "## Ground Truth Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7402d5",
   "metadata": {},
   "source": [
    "We won't spend a lot of time on this since we don't need to run this code frequently.\n",
    "\n",
    "First, let's extract all the code from the notebook:\n",
    "\n",
    "```bash\n",
    "jupyter nbconvert --to=script ground-truth.ipynb\n",
    "mv ground-truth.py ground_truth_generator.py\n",
    "```\n",
    "\n",
    "This converts our Jupyter notebook into a Python script for easier refactoring.\n",
    "\n",
    "Now ask ChatGPT to refactor the code:\n",
    "\n",
    "```text\n",
    "Make sure all the code is organized into separate functions with no global variables.\n",
    "```\n",
    "\n",
    "[Here's my refactoring conversation.](https://chatgpt.com/share/68f7454c-a2b8-800a-a811-0af5eb045e5a)\n",
    "\n",
    "In the video, I use GitHub Copilot for this refactoring process.\n",
    "\n",
    "You can see the final results here: https://github.com/alexeygrigorev/ai-bootcamp-codespace/blob/main/week3/code/evals/generate_data.py\n",
    "\n",
    "We can also create a simple script for sampling data (ground_truth_sample.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd074b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_ground_truth = pd.read_csv('ground_truth_evidently.csv')\n",
    "df_sample = df_ground_truth.sample(n=50)\n",
    "\n",
    "df_sample.to_csv('ground_truth_evidently_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aba492",
   "metadata": {},
   "source": [
    "This script creates smaller samples from our full dataset for faster testing and development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b5bb69",
   "metadata": {},
   "source": [
    "## Agent Application Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184eb0ad",
   "metadata": {},
   "source": [
    "Now let's handle step 1 of the evaluation part: applying the model to our ground truth data.\n",
    "\n",
    "We'll save the output as pickle files for persistence and debugging.\n",
    "\n",
    "First, convert the notebook to a Python script:\n",
    "\n",
    "```bash\n",
    "jupyter nbconvert --to=script 22-llm-ground-truth.ipynb\n",
    "mv 22-llm-ground-truth.py eval_apply.py\n",
    "```\n",
    "\n",
    "Now we need to clean up the experimental code. We did a lot of experimenting in the notebook, so we can delete everything we don't need.\n",
    "\n",
    "After refactoring, we end up with two organized files:\n",
    "\n",
    "- [eval_common.py](https://github.com/alexeygrigorev/ai-bootcamp-codespace/blob/main/week3/code/evals/eval_common.py) - shared utilities\n",
    "\n",
    "- [eval_agent_run.py](https://github.com/alexeygrigorev/ai-bootcamp-codespace/blob/main/week3/code/evals/eval_agent_run.py) - agent execution logic\n",
    "\n",
    "[Here's my conversation with ChatGPT about this refactoring.](https://chatgpt.com/share/68f74adf-438c-800a-a59a-713aa863dcc8) In the video, I used Copilot for the same process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee41a2",
   "metadata": {},
   "source": [
    "## Judge Evaluation Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0810d0dd",
   "metadata": {},
   "source": [
    "The first step is now organized. Let's add the second - the actual evaluation logic:\n",
    "\n",
    "```bash\n",
    "jupyter nbconvert --to=script 23-llm-judge-eval.ipynb\n",
    "mv 23-llm-judge-eval.py eval_judge.py\n",
    "```\n",
    "\n",
    "This extracts our LLM judge evaluation code into a proper Python module.\n",
    "\n",
    "My refactoring prompt for this step:\n",
    "\n",
    "```text\n",
    "Most of this code should go to eval_agent_judge.py with the evaluation logic.\n",
    "It should save the report to the reports folder.\n",
    "\n",
    "We already have the utils file, so use it for repeated functionality.\n",
    "Finally, create a simple script eval_orchestrator.py which will be a CLI app.\n",
    "This orchestrator puts everything together: first applying the model (eval_agent_run.py) \n",
    "then evaluating the model (eval_agent_judge.py).\n",
    "```\n",
    "\n",
    "The refactoring produces two files:\n",
    "\n",
    "- [eval_agent_judge.py](https://github.com/alexeygrigorev/ai-bootcamp-codespace/blob/main/week3/code/evals/eval_agent_judge.py) - LLM judge evaluation logic\n",
    "- [eval_orchestrator.py](https://github.com/alexeygrigorev/ai-bootcamp-codespace/blob/main/week3/code/evals/eval_orchestrator.py) - CLI orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73724ad6",
   "metadata": {},
   "source": [
    "## GitHub Copilot Refactoring Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ff1637",
   "metadata": {},
   "source": [
    "In the video, I used GitHub Copilot for refactoring the code.\n",
    "\n",
    "Here's my prompt from the video:\n",
    "\n",
    "```text\n",
    "I have two files that were jupyter notebooks: evals/eval_agent_judge.py evals/eval_agent_run.py\n",
    "\n",
    "Now I want to refactor these files: organize code into functions, make sure there are \n",
    "no global variables and that the code is modular.\n",
    "\n",
    "I also want to create two new files:\n",
    "\n",
    "First one with common utilities for both judge and run.\n",
    "Second for orchestrating the evaluation - running both of them: first run and then judge, \n",
    "and showing a nice report (with price for both steps).\n",
    "\n",
    "Note: the cost object returned from toyaikit is not a float, it's an object\n",
    "\n",
    "@dataclass\n",
    "class CostInfo:\n",
    "    input_cost: float\n",
    "    output_cost: float\n",
    "    total_cost: float\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd283a2",
   "metadata": {},
   "source": [
    "## Running the Complete Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e00709a",
   "metadata": {},
   "source": [
    "Now we can run the entire evaluation pipeline with simple commands:\n",
    "\n",
    "Create a sample dataset:\n",
    "\n",
    "```bash\n",
    "uv run python -m evals.sample_ground_truth \\\n",
    "    --sample-size 25 \\\n",
    "    --extra-indices 150 \\\n",
    "    --input evals/ground_truth_evidently.csv \\\n",
    "    --output=evals/gt-sample.csv\n",
    "```\n",
    "\n",
    "This creates a manageable sample from our full ground truth dataset.\n",
    "\n",
    "Run the complete evaluation:\n",
    "\n",
    "```bash\n",
    "uv run python -m evals.eval_orchestrator \\\n",
    "    --csv evals/gt-sample.csv \n",
    "```\n",
    "\n",
    "This command runs both the agent application and judge evaluation steps automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f39960",
   "metadata": {},
   "source": [
    "## Sample Evaluation Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480b67d0",
   "metadata": {},
   "source": [
    "Here's an example report (for 5 examples):\n",
    "\n",
    "```text\n",
    "Evaluation Report\n",
    "==================\n",
    "Average scores:\n",
    "instructions_follow    0.8\n",
    "instructions_avoid     1.0\n",
    "answer_relevant        1.0\n",
    "answer_clear           1.0\n",
    "answer_citations       0.6\n",
    "completeness           0.2\n",
    "tool_call_search       1.0\n",
    "\n",
    "Total Evaluation Cost: $0.02\n",
    "Samples Evaluated: 5\n",
    "```\n",
    "\n",
    "The report provides clear metrics and cost tracking for each evaluation run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3880ed55",
   "metadata": {},
   "source": [
    "## Benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73d4c3",
   "metadata": {},
   "source": [
    "Now we can easily integrate this into CI/CD pipelines. We can regularly monitor if any of these criteria scores decline over time.\n",
    "\n",
    "But we can use this framework for much more than monitoring. We can compare different approaches and see which one works best.\n",
    "\n",
    "For example, we could test different chunking strategies, prompt variations, or model configurations. Each approach can be evaluated systematically using the same framework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
