{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae148cf1",
   "metadata": {},
   "source": [
    "# Preparing for Evaluation: Running and Debugging the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a820f0a0",
   "metadata": {},
   "source": [
    "We'll prepare our evaluations: we'll run our agent using the generated ground truth data and analyze its performance patterns.\n",
    "\n",
    "In the next lesson, we'll use the results to run the actual evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29cfb69",
   "metadata": {},
   "source": [
    "## Setting Up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6836c4d",
   "metadata": {},
   "source": [
    "First, let's import our agent from the main module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ade90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import main\n",
    "agent = main.agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8838f2",
   "metadata": {},
   "source": [
    "This gives us access to the same agent we've been testing and improving throughout the module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c376803",
   "metadata": {},
   "source": [
    "## Loading Ground Truth Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f9dcf",
   "metadata": {},
   "source": [
    "Load the synthetic dataset we created in the previous lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad69dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_ground_truth = pd.read_csv('ground_truth_evidently.csv')\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622b6a23",
   "metadata": {},
   "source": [
    "Let's examine a sample question to understand what we're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5e878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = ground_truth[10]\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf15233",
   "metadata": {},
   "source": [
    "Let's test our agent with this sample question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0183f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await agent.run(q['question'])\n",
    "print(result.output.format_article())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccdfef1",
   "metadata": {},
   "source": [
    "## Preparing Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d89225",
   "metadata": {},
   "source": [
    "Intead of working on the entire dataset, we also can work on a sample.\n",
    "\n",
    "Let's select 50 questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d4b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "ground_truth_sample = random.sample(ground_truth, 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd12a4",
   "metadata": {},
   "source": [
    "Save the sample for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('sample.bin', 'wb') as f_out:\n",
    "    pickle.dump(ground_truth_sample, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238fa9af",
   "metadata": {},
   "source": [
    "## Error Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6778458c",
   "metadata": {},
   "source": [
    "The plan is to evaluate the agent against all ground truth data.\n",
    "\n",
    "But what if it breaks while evaluating? It'd be pity if at 80% it breaks with a network error (timeout or something like that), and we need to re-run the whole thing .\n",
    "\n",
    "So let's put things into a try/except block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3cefda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "async def run_agent(q):\n",
    "    try:\n",
    "        result = await agent.run(q['question'])\n",
    "        return (q, result)\n",
    "    except:\n",
    "        print(f'error processing {q}')\n",
    "        traceback.print_exc()\n",
    "        return (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a685b02e",
   "metadata": {},
   "source": [
    "This wrapper ensures that even if some queries fail (due to token limits or other issues), we can continue processing the remaining questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909447e8",
   "metadata": {},
   "source": [
    "## Parallel Processing Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c73cf3",
   "metadata": {},
   "source": [
    "To efficiently process multiple queries, we'll use asynchronous processing (ChatGPT helped me translate the ThreadPoolExecutor version into asyncio):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d90bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "async def map_progress(seq, f, max_concurrency=6):\n",
    "    \"\"\"Asynchronously map async function f over seq with progress bar.\"\"\"\n",
    "    semaphore = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "    async def run(el):\n",
    "        async with semaphore:\n",
    "            return await f(el)\n",
    "\n",
    "    # create one coroutine per element\n",
    "    coros = [run(el) for el in seq]\n",
    "\n",
    "    # turn them into tasks that complete as they finish\n",
    "    completed = asyncio.as_completed(coros)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for coro in tqdm(completed, total=len(seq)):\n",
    "        result = await coro\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2589dc4",
   "metadata": {},
   "source": [
    "## Running the Initial Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576846fc",
   "metadata": {},
   "source": [
    "Execute the evaluation on our sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = await map_progress(ground_truth_sample, run_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea6a4bb",
   "metadata": {},
   "source": [
    "Analyze the cost of running this evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c90dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.pricing import PricingConfig\n",
    "pricing = PricingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad86cad",
   "metadata": {},
   "source": [
    "Create a helper function to calculate total costs across all results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(model, all_results):\n",
    "    total_input = 0\n",
    "    total_output = 0\n",
    "    \n",
    "    for q, result in all_results:\n",
    "        if result is None:\n",
    "            continue\n",
    "        usage = result.usage()\n",
    "        total_input = total_input + usage.input_tokens\n",
    "        total_output = total_output + usage.output_tokens\n",
    "    \n",
    "    return pricing.calculate_cost(model, total_input, total_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25871f58",
   "metadata": {},
   "source": [
    "Check the cost for our sample run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2647eb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_cost('gpt-4o-mini', all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64e53ee",
   "metadata": {},
   "source": [
    "It's a few cents. For full dataset it'd be around $1.0-$1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33db34b6",
   "metadata": {},
   "source": [
    "## Processing Results for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b5880",
   "metadata": {},
   "source": [
    "When we run it on many queries, we can spot some problems. For example, for some queries the agent is making too many search queries.\n",
    "\n",
    "Let's look into it.\n",
    "\n",
    "First, create a helper functions to simplify the message structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def simplify_messages(messages):\n",
    "    messages_simplified = []\n",
    "\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "\n",
    "        for original_part in m.parts:\n",
    "            kind = original_part.part_kind\n",
    "            # print(original_part)\n",
    "            part = {\n",
    "                'kind': kind\n",
    "            }\n",
    "            if kind == 'user-prompt':\n",
    "                part['content'] = original_part.content\n",
    "            if kind == 'tool-call':\n",
    "                if original_part.tool_name == 'final_result':\n",
    "                    continue\n",
    "    \n",
    "                part['tool_name'] = original_part.tool_name\n",
    "                part['args'] = json.loads(original_part.args)\n",
    "            if kind == 'tool-return':\n",
    "                continue\n",
    "            if kind == 'text':\n",
    "                part['content'] = original_part.content\n",
    "\n",
    "            parts.append(part)\n",
    "\n",
    "        if len(parts) > 0:\n",
    "            messages_simplified.extend(parts)\n",
    "\n",
    "    return messages_simplified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f2c3c",
   "metadata": {},
   "source": [
    "This function extracts essential information from the complex message structure, filtering out internal tool calls like final_result. Otherwise it will be too much stuff to look at.\n",
    "\n",
    "Now let's count the number of tool calls to understand agent behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65ab0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tool_calls(messages):\n",
    "    cnt = 0 \n",
    "    for m in messages:\n",
    "        if m['kind'] == 'tool-call':\n",
    "            cnt = cnt + 1\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f677b6",
   "metadata": {},
   "source": [
    "Let's process all the records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4230d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_result(q, result):\n",
    "    row = {}\n",
    "\n",
    "    row['question'] = q['question']\n",
    "    row['answer'] = result.output.format_article()\n",
    "    row['messages'] = simplify_messages(result.new_messages())\n",
    "    row['num_tool_calls'] = count_tool_calls(row['messages']) \n",
    "\n",
    "    row['original_question'] = q\n",
    "    row['original_result'] = result\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "for q, result in all_results:\n",
    "    if result is None:\n",
    "        continue\n",
    "\n",
    "    row = process_result(q, result)\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caf4080",
   "metadata": {},
   "source": [
    "Put everything inside a pandas DataFrame for easier analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c218cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logs = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6814e48e",
   "metadata": {},
   "source": [
    "## Identifying Performance Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c28ae8",
   "metadata": {},
   "source": [
    "During our analysis, we discovered a problem: When it can't find something, it keeps searching and searching.\n",
    "\n",
    "We need to stop it and just explicitly say: \"can't find the information you're asking\". To address it, we'll ask it to limit search to 6 queries. If it can't find anything, then we'll ask it to just say it.\n",
    "\n",
    "Let's go back to the agent code and update the instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec40114",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_instructions = \"\"\"\n",
    "You are a search assistant for the Evidently documentation.\n",
    "\n",
    "Requirements:\n",
    "- For every user query, you must perform at least 3 and at most 6 separate searches\n",
    "- Keep all searches relevant to Evidently and centered on technical or conceptual details\n",
    "- The FAQ database contains only Evidently-related content, so you don't need to include \"Evidently\" in queries\n",
    "- If you cannot answer the user's question after 6 searches, set `found_answer` to False\n",
    "- Do not perform more than 6 searches per query\n",
    "\"\"\"\n",
    "\n",
    "class SearchResultArticle(BaseModel):\n",
    "    found_answer: bool # <- this\n",
    "    title: str\n",
    "    sections: list[Section]\n",
    "    references: list[Reference]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a61f274",
   "metadata": {},
   "source": [
    "Let's add this to the tests we created previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85e5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_doesnt_make_too_many_calls():\n",
    "    result = run_agent_sync(\"detecting constant features in dataset\")\n",
    "\n",
    "    tool_calls = get_tool_calls(result)\n",
    "\n",
    "    assert len(tool_calls) >= 3, \"Less than 3 tool calls found\"\n",
    "    assert len(tool_calls) <= 10, \"More than 10 tool calls found\"\n",
    "\n",
    "    output = result.output\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f961cba",
   "metadata": {},
   "source": [
    "And this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc889ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.asyncio\n",
    "async def test_cannot_find_result():\n",
    "    criteria = [\n",
    "        \"agent makes between 3 and 10 search calls\",\n",
    "        \"article should say that no relevant information was found\",\n",
    "    ]\n",
    "\n",
    "    result = await run_agent(\"detecting constant features in dataset\")\n",
    "    print(result.output.format_article())\n",
    "\n",
    "    eval_results = await evaluate_agent_performance(\n",
    "        criteria,\n",
    "        result,\n",
    "        output_transformer=lambda x: x.format_article()\n",
    "    )\n",
    "\n",
    "    print(eval_results)\n",
    "\n",
    "    for criterion in eval_results.criteria:\n",
    "        print(criterion)\n",
    "        assert criterion.passed, f\"Criterion failed: {criterion.criterion_description}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa08ee67",
   "metadata": {},
   "source": [
    "## Re-running Evaluation with Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8b49e",
   "metadata": {},
   "source": [
    "After implementing improvements, re-run the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bec7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = await map_progress(ground_truth_sample, run_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6810473e",
   "metadata": {},
   "source": [
    "Sort by number of tool calls to understand search patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e69e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logs.sort_values(by='num_tool_calls')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb3aa5",
   "metadata": {},
   "source": [
    "## Advanced Search Control Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e32da4c",
   "metadata": {},
   "source": [
    "For even better control, we can programmatically limit searches. Let's programmatically forbid it making many calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccbad5a",
   "metadata": {},
   "source": [
    "### Option 1: Context-aware search limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d9776",
   "metadata": {},
   "source": [
    "We wrap our search function inside another one that has access to RunContext. Through it, we can see how many run_steps (tool-call loop iterations) we already made, and if it's too many, don't return anything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a14247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent():\n",
    "    tools = search_tools.prepare_search_tools()\n",
    "\n",
    "    def search(ctx: RunContext, query: str):\n",
    "        if ctx.run_step > 3:\n",
    "            return {\"message\": \"Maximum number of searches reached.\"}\n",
    "\n",
    "        return tools.search(query)\n",
    "\n",
    "    search.__doc__ = tools.search.__doc__\n",
    "\n",
    "    return Agent(\n",
    "        name=\"search\",\n",
    "        instructions=search_instructions,\n",
    "        tools=[search],\n",
    "        model=\"openai:gpt-4o-mini\",\n",
    "        output_type=SearchResultArticle,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6051213",
   "metadata": {},
   "source": [
    "Note: we need to copy the docstrings to the new search tool, otherwise the LLM won't know how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb089e2",
   "metadata": {},
   "source": [
    "### Option 2: Message history processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d9c7a3",
   "metadata": {},
   "source": [
    "Alternatively, we can alter message history using [history_processors](https://ai.pydantic.dev/message-history/). Here's the docstring from the Agent class:\n",
    "\n",
    "```text\n",
    "history_processors: Optional list of callables to process the message history before sending it to the model.\n",
    "            Each processor takes a list of messages and returns a modified list of messages.\n",
    "            Processors can be sync or async and are applied in sequence.\n",
    "```\n",
    "\n",
    "Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43ecf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.messages import ModelMessage, UserPromptPart\n",
    "\n",
    "def force_answer_after_6_searches(messages: list[ModelMessage]) -> list[ModelMessage]: \n",
    "    num_tool_calls = 0\n",
    "    \n",
    "    for m in messages:\n",
    "        for p in m.parts:\n",
    "            if p.part_kind == 'tool-call' and p.tool_name == 'search':\n",
    "                num_tool_calls = num_tool_calls + 1\n",
    "\n",
    "    if num_tool_calls >= 6:\n",
    "        print('forcing output')\n",
    "        last_message = messages[-1]\n",
    "        finish_prompt = 'System message: The maximal number of searches has exceeded 6. Proceed to finishing the writeup'\n",
    "        finish_prompt_part = UserPromptPart(finish_prompt)\n",
    "        last_message.parts.append(finish_prompt_part)\n",
    "\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274ce6e",
   "metadata": {},
   "source": [
    "Now we control the agent by injecting instructions to stop using search and proceed to generating the answer.\n",
    "\n",
    "This option is a little cleaner as we don't need to modify our tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9601b62",
   "metadata": {},
   "source": [
    "## Final Evaluation Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9346f36f",
   "metadata": {},
   "source": [
    "Execute the final evaluation with all improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba1f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = await map_progress(ground_truth_sample, run_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf34f61",
   "metadata": {},
   "source": [
    "Check the final costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee6b0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_cost('gpt-4o-mini', all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe838485",
   "metadata": {},
   "source": [
    "Process and save the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36e6a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for q, result in all_results:\n",
    "    if result is None:\n",
    "        continue\n",
    "\n",
    "    row = process_result(q, result)\n",
    "    rows.append(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f240a9d",
   "metadata": {},
   "source": [
    "And save it for the next step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d43d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample_eval_rows.bin', 'wb') as f_out:\n",
    "    pickle.dump(rows, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0864ba35",
   "metadata": {},
   "source": [
    "## Summary and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaced436",
   "metadata": {},
   "source": [
    "In this lesson we run the agent and collected the output. We also identified a few problems and fixed them.\n",
    "\n",
    "Because we run the agent against a lot of queries, we can easily identify some common problems.\n",
    "\n",
    "Now we can use the data we collected to actually do the evaluation. We will do it in the next lesson."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
