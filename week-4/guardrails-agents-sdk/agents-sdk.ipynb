{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2007f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docs\n",
    "import search_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c0e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b60220",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AgentConfig:\n",
    "    chunk_size: int = 2000\n",
    "    chunk_step: int = 1000\n",
    "    top_k: int = 5\n",
    "\n",
    "    model: str = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9402306",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_instructions = \"\"\"\n",
    "You are a search assistant for the Evidently documentation.\n",
    "\n",
    "Evidently is an open-source Python library and cloud platform for evaluating, testing, and monitoring data and AI systems.\n",
    "It provides evaluation metrics, testing APIs, and visual reports for model and data quality.\n",
    "\n",
    "Your task is to help users find accurate, relevant information about Evidently's features, usage, and integrations.\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "- search — Use this to explore the topic and retrieve relevant snippets or documentation.\n",
    "- read_file — Use this to retrieve or verify the complete content of a file when:\n",
    "    * A code snippet is incomplete, truncated, or missing definitions.\n",
    "    * You need to check that all variables, imports, and functions referenced in code are defined.\n",
    "    * You must ensure the code example is syntactically correct and runnable.\n",
    "\n",
    "If `read_file` cannot be used or the file content is unavailable, clearly state:\n",
    "> \"Unable to verify with read_file.\"\n",
    "\n",
    "Search Strategy\n",
    "\n",
    "- For every user query:\n",
    "    * Perform at least 3 and at most 6 distinct searches to gather enough context.\n",
    "    * Each search must use a different phrasing or keyword variation of the user's question.\n",
    "    * Make sure that the search requests are relevant to evidently, testing, evaluating and monitoring AI systems.\n",
    "    * No need to include \"Evidently\" in the search text.\n",
    "\n",
    "- After collecting search results:\n",
    "    1. Synthesize the information into a concise, accurate answer.\n",
    "    2. If your answer includes code, always validate it with `read_file` before finalizing.\n",
    "    3. If a code snippet or reference is incomplete, explicitly mention it.\n",
    "\n",
    "Important:\n",
    "- The 6-search limit applies only to `search` calls.\n",
    "- You may call `read_file` at any time, even after the search limit is reached.\n",
    "- `read_file` calls are verification steps and do not count toward the 6-search limit.\n",
    "\n",
    "Code Verification and Completeness Rules\n",
    "\n",
    "- All variables, functions, and imports in your final code examples must be defined or imported.\n",
    "- Never shorten, simplify, or truncate code examples. Always present the full, verified version.\n",
    "- When something is missing or undefined in the search results:\n",
    "    * Call `read_file` with the likely filename to retrieve the complete file content.\n",
    "    * Replace any partial code with the full verified version.\n",
    "- If the file is not available or cannot be verified:\n",
    "    * Include a clear note: \"Unable to verify this code.\"\n",
    "- Do not reformat, rename variables, or omit lines from the verified code.\n",
    "\n",
    "Output Format\n",
    "\n",
    "- Write your answer clearly and accurately.\n",
    "- Include a \"References\" section listing the search queries or file names you used.\n",
    "- If you couldn't find a complete answer after 6 searches, set found_answer = False.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "841dba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reference(BaseModel):\n",
    "    title: str\n",
    "    filename: str\n",
    "\n",
    "class Section(BaseModel):\n",
    "    heading: str\n",
    "    content: str\n",
    "    references: list[Reference]\n",
    "\n",
    "class SearchResultArticle(BaseModel):\n",
    "    found_answer: bool\n",
    "    title: str\n",
    "    sections: list[Section]\n",
    "    references: list[Reference]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb750239",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AgentConfig()\n",
    "\n",
    "tools = search_tools.prepare_search_tools(\n",
    "    config.chunk_size,\n",
    "    config.chunk_step,\n",
    "    config.top_k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbba2081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, function_tool\n",
    "\n",
    "agent_tools = [\n",
    "    function_tool(tools.search),\n",
    "    function_tool(tools.read_file)\n",
    "]\n",
    "\n",
    "search_agent = Agent(\n",
    "    name='search',\n",
    "    tools=agent_tools,\n",
    "    instructions=search_instructions,\n",
    "    model=config.model,\n",
    "    output_type=SearchResultArticle,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b32e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Runner\n",
    "\n",
    "input = 'data drift'\n",
    "result = await Runner.run(search_agent, input=input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c9e4ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SearchResultArticle(found_answer=True, title='Data Drift Overview', sections=[Section(heading='What is Data Drift?', content='Data drift refers to the changes in data distribution over time, which can impact the performance of machine learning models. It is essential to monitor data drift because a model trained on historical data may fail to perform as expected when deployed on new data that has shifted significantly from the training set.', references=[]), Section(heading='How to Detect Data Drift', content='Evidently provides several methods to detect data drift. The core of its functionality is the `DataDriftPreset` that evaluates shifts in data distribution between current and reference datasets. Here’s a basic example of how to use this:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(),\\n])\\n\\nmy_eval = report.run(current, reference)\\n```\\n\\nThis preset checks for:\\n- **Column Drift**: Evaluates each feature for shifts in distribution.\\n- **Target/Prediction Drift**: If the dataset includes a prediction target, it evaluates that too.\\n- **Overall Dataset Drift**: It summarizes how many columns showed drift, typically marking a dataset as drifted if at least 50% of the columns drifted.\\n\\n### Statistical Tests Employed\\nEvidently leverages various statistical tests depending on:\\n- Column type (categorical, numerical, text)\\n- Number of unique values\\n- Sample size of the reference dataset', references=[Reference(title='Data Drift Methodology', filename='metrics/preset_data_drift.mdx')]), Section(heading='Use Cases for Monitoring Data Drift', content=\"Monitoring data drift can help in several scenarios:\\n- **Model Performance Monitoring**: Helps to observe changes in model behavior when true labels are not available.\\n- **Debugging Model Quality Decay**: Assists in understanding why a model's performance may decline over time by correlating it with changes in input data.\\n- **Historical Analysis**: Allows review of past data drift to refine models and retraining strategies.\\n- **Decision Making for Retraining**: Aids in determining when to retrain models based on stability indicated by the absence of drift.\", references=[]), Section(heading='Customization Options', content='Evidently allows the customization of drift detection parameters:\\n- You can choose specific drift detection methods (e.g., K-L divergence, Jensen-Shannon distance, etc.).\\n- You can set specific thresholds for drift detection on a dataset or column level.\\n- Implement custom drift detection methods.\\n\\n### Additional Measurement\\nYou can also incorporate data quality assessments alongside data drift checks to get a more holistic view of model performance and data integrity.', references=[Reference(title='Explaining Data Drift Detection', filename='metrics/explainer_drift.mdx')])], references=[Reference(title='Data Drift Detection Methods', filename='metrics/preset_data_drift.mdx'), Reference(title='How Data Drift Works', filename='metrics/explainer_drift.mdx')])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e40f7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (search): search({\"query\":\"data drift in machine learning\"})\n",
      "TOOL CALL (search): search({\"query\":\"how to detect data drift\"})\n",
      "TOOL CALL (search): search({\"query\":\"data drift monitoring techniques\"})\n",
      "TOOL CALL (search): search({\"query\":\"impact of data drift on AI models\"})\n",
      "TOOL CALL (search): search({\"query\":\"strategies to handle data drift\"})\n",
      "TOOL CALL (search): read_file({\"filename\":\"metrics/explainer_drift.mdx\"})\n",
      "TOOL CALL (search): read_file({\"filename\":\"metrics/preset_data_drift.mdx\"})\n",
      "{\"found_answer\":true,\"title\":\"Data Drift Overview\",\"sections\":[{\"heading\":\"What is Data Drift?\",\"content\":\"Data drift refers to changes in the statistical properties of a dataset over time. This phenomenon can occur when the data used to train a machine learning model no longer reflects the characteristics of the current operational environment, potentially leading to a degradation in model performance.\",\"references\":[]},{\"heading\":\"How Data Drift Works in Evidently\",\"content\":\"Evidently detects data drift by comparing the distributions of values in specified columns of two datasets—referred to as the **current** and **reference** datasets. The library applies various statistical tests to identify significant distribution changes, returning a result indicating whether drift is detected or not.\\n\\nKey factors governing this detection include:\\n- **Column type** (categorical, numerical, text data)\\n- **Number of observations** in the reference dataset\\n- **Number of unique values** in the column.\" ,\"references\":[]},{\"heading\":\"Data Requirements\",\"content\":\"To evaluate data drift, you need to provide:\\n- **Two datasets:** The current dataset that's evaluated and the reference dataset used as a benchmark.\\n- **Non-empty columns:** Ensure the columns tested for drift are not empty; empty columns will result in errors during analysis.\\n\\nAdditionally, you may optionally set column types, wherein default types can be auto-detected by Evidently.\\n\",\"references\":[]},{\"heading\":\"Use Cases for Monitoring Data Drift\",\"content\":\"Data drift evaluation can assist in several scenarios:\\n1. **Monitoring model performance** without ground truth: If true labels are unavailable, monitoring feature and prediction drift can indicate if a model continues to function effectively in its environment.\\n2. **Debugging model quality decay:** If model performance declines, drift evaluation can help identify feature pattern changes.\\n3. **Understanding offline model drift:** Examining past data drift allows for optimal drift detection methods and retraining strategies.\\n4. **Deciding on model retraining:** Verifying the necessity of retraining based on the presence of drift helps in maintaining a stable environment before introducing new data.\",\"references\":[]},{\"heading\":\"Configuration and Customization\",\"content\":\"Evidently allows for substantial customization in drift detection:\\n- **Choosing methods:** Utilize various drift detection methods (e.g., PSI, K-L divergence) based on the dataset's attributes.\\n- **Setting thresholds** for drift detection can be done at either column or dataset levels.\\n- **Implementing custom methods** or passing custom tests enhances flexibility.\",\"references\":[]},{\"heading\":\"Resources\",\"content\":\"For a more comprehensive understanding of data drift and detection techniques, you can refer to:\\n- [Data Drift Detection](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets) blog.\\n- [Embedding Drift Detection](https://www.evidentlyai.com/blog/embedding-drift-detection) blog.\\n- [How to interpret data and prediction drift](https://evidentlyai.com/blog/data-and-prediction-drift) guide.\",\"references\":[]}],\"references\":[{\"title\":\"How data drift detection works\",\"filename\":\"metrics/explainer_drift.mdx\"},{\"title\":\"Overview of the Data Drift Preset\",\"filename\":\"metrics/preset_data_drift.mdx\"}]}"
     ]
    }
   ],
   "source": [
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "\n",
    "result = Runner.run_streamed(\n",
    "    search_agent,\n",
    "    input=input,\n",
    "    max_turns=3\n",
    ")\n",
    "\n",
    "async for event in result.stream_events():\n",
    "    if event.type == \"run_item_stream_event\":\n",
    "        if event.item.type == \"tool_call_item\":\n",
    "            tool_call = event.item.raw_item\n",
    "            f_name = tool_call.name\n",
    "            args = tool_call.arguments\n",
    "            print(f\"TOOL CALL ({event.item.agent.name}): {f_name}({args})\")\n",
    "    \n",
    "    if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "        print(event.data.delta, end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fea857e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxn import StreamingJSONParser, JSONParserHandler\n",
    "\n",
    "class SearchResultHandler(JSONParserHandler):\n",
    "    def on_field_start(self, path: str, field_name: str):\n",
    "        if field_name == \"references\":\n",
    "            level = path.count(\"/\") + 2\n",
    "            print(f\"\\n{'#' * level} References\\n\")\n",
    "\n",
    "    def on_field_end(self, path, field_name, value, parsed_value=None):\n",
    "        if field_name == \"title\" and path == \"\":\n",
    "            print(f\"# {value}\")\n",
    "\n",
    "        elif field_name == \"heading\":\n",
    "            print(f\"\\n\\n## {value}\\n\")\n",
    "        elif field_name == \"content\":\n",
    "            print(\"\\n\") \n",
    "\n",
    "    def on_value_chunk(self, path, field_name, chunk):\n",
    "        if field_name == \"content\":\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "\n",
    "    def on_array_item_end(self, path, field_name, item=None):\n",
    "        if field_name == \"references\":\n",
    "            title = item.get(\"title\", \"\")\n",
    "            filename = item.get(\"filename\", \"\")\n",
    "            print(f\"- [{title}]({filename})\")\n",
    "\n",
    "handler = SearchResultHandler()\n",
    "parser = StreamingJSONParser(handler)\n",
    "\n",
    "# Parse each chunk as it arrives\n",
    "# parser.parse_incremental(event.data.delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c077e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (search): search({\"query\":\"data drift monitoring in machine learning\"})\n",
      "TOOL CALL (search): search({\"query\":\"how to detect data drift in models\"})\n",
      "TOOL CALL (search): search({\"query\":\"data drift evaluation metrics\"})\n",
      "TOOL CALL (search): search({\"query\":\"preventing data drift in AI systems\"})\n",
      "TOOL CALL (search): search({\"query\":\"data drift examples in AI\"})\n",
      "TOOL CALL (search): read_file({\"filename\":\"metrics/explainer_drift.mdx\"})\n",
      "# Understanding Data Drift\n",
      "\n",
      "\n",
      "## What is Data Drift?\n",
      "\n",
      "Data drift refers to changes in the statistical properties of the data that a machine learning model is trained on. It can have a significant impact on model performance and can occur over time due to various reasons such as changing environments, new trends, or altered user behavior.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "\n",
      "\n",
      "## How Data Drift Detection Works\n",
      "\n",
      "Evidently provides a default Data Drift Detection algorithm that compares the distributions of values in specified columns between two datasets, typically a current dataset and a reference (training) dataset. It applies several statistical tests to determine if significant changes have occurred:\n",
      "\n",
      "- **Column Type**: The algorithm chooses different tests based on whether the data is categorical, numerical, or text data.\n",
      "- **Number of Observations**: It considers how many observations are in the reference dataset.\n",
      "- **Unique Values**: The test used may also depend on the number of unique values within the column.\n",
      "\n",
      "The method will return either \"drift detected\" or \"not detected\" results, based on the statistical findings.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "- [Evidently Data Drift Detection](metrics/explainer_drift.mdx)\n",
      "\n",
      "\n",
      "## Data Requirements\n",
      "\n",
      "To evaluate data drift, you must provide:\n",
      "\n",
      "- Two datasets: the current dataset (the one being evaluated) and the reference dataset which serves as a benchmark.\n",
      "- Ensure the columns being tested are not empty, as empty columns will cause errors in drift calculations.\n",
      "\n",
      "Evidently filters out any empty or infinite values when calculating distribution drift.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "- [Data Requirements in Evidently](metrics/explainer_drift.mdx)\n",
      "\n",
      "\n",
      "## Detection Approaches by Data Size\n",
      "\n",
      "The detection method varies based on the size of the reference dataset:\n",
      "\n",
      "- **For small datasets (<= 1000 observations)**: Employs methods like Kolmogorov-Smirnov test for numeric columns and the chi-squared test for categorical columns.\n",
      "- **For larger datasets (> 1000 observations)**: Utilizes Wasserstein distance for numeric columns and Jensen-Shannon divergence for categorical columns.\n",
      "\n",
      "You can modify the detection logic to use specific methods or specify thresholds based on your needs.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "\n",
      "\n",
      "## Text Data Drift\n",
      "\n",
      "For text data, Evidently uses a domain classifier approach to detect drift. This includes training a binary classifier that distinguishes between text samples from the reference and current datasets. If the model confidently identifies samples from the current dataset as distinct, drift is considered to have occurred. The ROC AUC score of the classifier guides the detection process.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "\n",
      "\n",
      "## Next Steps and Resources\n",
      "\n",
      "If you suspect data drift, you might want to consider:\n",
      "- Monitoring model performance without ground truth by looking at feature and prediction drift some proxy metrics.\n",
      "- Triggering retraining or labelling processes based on detected drifts.\n",
      "\n",
      "For more details and practical implementations, check the following resources:\n",
      "- [Best Tests for Data Drift Detection](https://evidentlyai.com/blog/data-drift-detection-large-datasets)\n",
      "- [How to Handle Data Drift](https://evidentlyai.com/blog/ml-monitoring-data-drift-how-to-handle)\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "\n",
      "## References\n",
      "\n",
      "- [How data drift detection works](metrics/explainer_drift.mdx)\n",
      "- [Overview of the Data Drift Preset](metrics/preset_data_drift.mdx)\n"
     ]
    }
   ],
   "source": [
    "result = Runner.run_streamed(\n",
    "    search_agent,\n",
    "    input=input,\n",
    "    max_turns=3\n",
    ")\n",
    "\n",
    "parser = StreamingJSONParser(handler)\n",
    "\n",
    "async for event in result.stream_events():\n",
    "    if event.type == \"run_item_stream_event\":\n",
    "        if event.item.type == \"tool_call_item\":\n",
    "            tool_call = event.item.raw_item\n",
    "            f_name = tool_call.name\n",
    "            args = tool_call.arguments\n",
    "            print(f\"TOOL CALL ({event.item.agent.name}): {f_name}({args})\")\n",
    "    \n",
    "    if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "        parser.parse_incremental(event.data.delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6da9bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 'llm as a judge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2c3f08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (search): search({\"query\":\"using LLM as a judge in legal contexts\"})\n",
      "TOOL CALL (search): search({\"query\":\"AI judge applications legal system\"})\n",
      "TOOL CALL (search): search({\"query\":\"legal decision making AI models\"})\n",
      "TOOL CALL (search): search({\"query\":\"AI in court decisions\"})\n",
      "TOOL CALL (search): search({\"query\":\"machine learning models for judicial decisions\"})\n",
      "TOOL CALL (search): read_file({\"filename\":\"examples/LLM_judge.mdx\"})\n",
      "TOOL CALL (search): read_file({\"filename\":\"quickstart_llm.mdx\"})\n",
      "too many turns\n"
     ]
    }
   ],
   "source": [
    "from agents.exceptions import MaxTurnsExceeded\n",
    "\n",
    "try:\n",
    "    parser = StreamingJSONParser(handler)\n",
    "\n",
    "    result = Runner.run_streamed(\n",
    "        search_agent,\n",
    "        input=input,\n",
    "        max_turns=3\n",
    "    )\n",
    "\n",
    "    async for event in result.stream_events():\n",
    "        if event.type == \"run_item_stream_event\":\n",
    "            if event.item.type == \"tool_call_item\":\n",
    "                tool_call = event.item.raw_item\n",
    "                f_name = tool_call.name\n",
    "                args = tool_call.arguments\n",
    "                print(f\"TOOL CALL ({event.item.agent.name}): {f_name}({args})\")\n",
    "        \n",
    "        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            parser.parse_incremental(event.data.delta)\n",
    "except MaxTurnsExceeded as e:\n",
    "    print('too many turns')\n",
    "    finish_prompt = 'System message: The number of searches has exceeded the limit. Proceed to finishing the writeup'\n",
    "    finish_message = [{'role': 'user', 'content': finish_prompt}]\n",
    "    messages = result.to_input_list() + finish_message\n",
    "    # Run one more time with the accumulated messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d217169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "498a4730",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_stream(agent, input, handler, max_turns=3):\n",
    "    try:\n",
    "        result = Runner.run_streamed(\n",
    "            agent,\n",
    "            input=input,\n",
    "            max_turns=max_turns\n",
    "        )\n",
    "        \n",
    "        parser = StreamingJSONParser(handler)\n",
    "\n",
    "        async for event in result.stream_events():\n",
    "            if event.type == \"run_item_stream_event\":\n",
    "                if event.item.type == \"tool_call_item\":\n",
    "                    tool_call = event.item.raw_item\n",
    "                    f_name = tool_call.name\n",
    "                    args = tool_call.arguments\n",
    "                    print(f\"TOOL CALL ({event.item.agent.name}): {f_name}({args})\")\n",
    "            \n",
    "            if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "                parser.parse_incremental(event.data.delta)\n",
    "\n",
    "        return result\n",
    "    except MaxTurnsExceeded as e:\n",
    "        print('too many turns')\n",
    "        finish_prompt = 'System message: The number of searches has exceeded the limit. Proceed to finishing the writeup'\n",
    "        finish_message = [{'role': 'user', 'content': finish_prompt}]\n",
    "        messages = result.to_input_list() + finish_message\n",
    "        final_result = await run_stream(agent, input=messages, handler=handler, max_turns=1)\n",
    "        return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89909676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALL (search): search({\"query\":\"using AI as a judge in legal systems\"})\n",
      "TOOL CALL (search): search({\"query\":\"AI in judicial decision making\"})\n",
      "TOOL CALL (search): search({\"query\":\"machine learning algorithms in law\"})\n",
      "TOOL CALL (search): search({\"query\":\"automated legal systems AI\"})\n",
      "TOOL CALL (search): search({\"query\":\"ethical implications of AI judges\"})\n",
      "TOOL CALL (search): read_file({\"filename\":\"examples/LLM_judge.mdx\"})\n",
      "TOOL CALL (search): read_file({\"filename\":\"docs/platform/evals_no_code.mdx\"})\n",
      "TOOL CALL (search): read_file({\"filename\":\"metrics/customize_llm_judge.mdx\"})\n",
      "TOOL CALL (search): read_file({\"filename\":\"docs/platform/monitoring_overview.mdx\"})\n",
      "too many turns\n",
      "# Using LLMs as Judges\n",
      "\n",
      "\n",
      "## Overview\n",
      "\n",
      "AI language models (LLMs) can be effectively utilized as judges in evaluating text for various applications such as customer support, content moderation, and educational assessments. They enable automated evaluations that can save time and increase consistency in decision-making processes.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "- [LLM as a judge](examples/LLM_judge.mdx)\n",
      "- [No code evals](docs/platform/evals_no_code.mdx)\n",
      "- [Configure LLM Judges](metrics/customize_llm_judge.mdx)\n",
      "\n",
      "\n",
      "## Implementation\n",
      "\n",
      "To implement an LLM as a judge, one typically follows these steps:\n",
      "\n",
      "1. **Dataset Preparation**: Create a dataset with inputs (questions or text) and corresponding target responses. This should also include any prior manual evaluations for ground truth comparisons.\n",
      "2. **Model Import and Setup**: Import necessary libraries and set up the LLM environment with appropriate API keys.\n",
      "3. **Define Evaluation Criteria**: Create prompt templates that describe how the LLM should evaluate the text. This can include varying metrics such as correctness, conciseness, or relevance.\n",
      "4. **Run Evaluations**: Use descriptors to apply the LLM evaluations to the dataset, adding results back to the dataset for analysis.\n",
      "5. **Results Analysis**: Generate reports using the Evidently library to visualize and understand the evaluation outcomes, including metrics like accuracy and recall.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "- [Batch monitoring overview](docs/platform/monitoring_overview.mdx)\n",
      "- [LLM Evaluation](quickstart_llm.mdx)\n",
      "\n",
      "\n",
      "## Benefits\n",
      "\n",
      "Using LLMs as judges offers several advantages:\n",
      "- **Consistency**: Automated evaluations reduce variability in decisions compared to manual assessments.\n",
      "- **Scalability**: An LLM can handle large volumes of data quickly, making it suitable for high-demand applications.\n",
      "- **Flexibility**: Custom templates allow for tailored evaluations across various domains, from customer service to content generation.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "- [LLM Evaluation](quickstart_llm.mdx)\n",
      "- [Configure LLM Judges](metrics/customize_llm_judge.mdx)\n",
      "\n",
      "\n",
      "## Considerations\n",
      "\n",
      "While LLMs can be powerful tools, several considerations should be kept in mind:\n",
      "- **Context Sensitivity**: LLMs require carefully structured prompts to ensure accurate evaluations.\n",
      "- **Data Quality**: The results are heavily dependent on the quality and representativeness of the training data used to fine-tune the models.\n",
      "- **Ethical Implications**: Be aware of biases that may be present in LLMs and the need for continual monitoring and adjustment.\n",
      "\n",
      "\n",
      "### References\n",
      "\n",
      "- [Prompt optimization guidelines](docs/library/prompt_optimization.mdx)\n",
      "- [Toxicity detection](metrics/all_descriptors)\n",
      "\n",
      "## References\n",
      "\n",
      "- [LLM as a judge](examples/LLM_judge.mdx)\n",
      "- [No code evals](docs/platform/evals_no_code.mdx)\n",
      "- [Configure LLM Judges](metrics/customize_llm_judge.mdx)\n"
     ]
    }
   ],
   "source": [
    "result = await run_stream(search_agent, 'llm as a judge', SearchResultHandler())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
